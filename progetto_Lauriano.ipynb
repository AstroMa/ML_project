{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificazione di 5 classi di stelle per mezzo di curve di luce simulate\n",
    "Progetto per l'esame del corso *Machine Learning per la fisica* A.A. 2023-24 Prof. Donato Cascio\\\n",
    "Mario Lauriano (matr. 0767930) - LM Fisica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tale progetto si inserisce all'interno di una campagna osservativa in corso di stelle supergiganti rosse (RSG) guidata dal Dott. Fabrizio Bocchino di INAF-Osservatorio Astronomico di Palermo. Il dataset a mia disposizione comprende **10,000 curve di luce simulate** ed etichettate di RSG **suddivise in 5 classi** in base ai periodi di variazione della luminosità, come suggerito dalla letteratura. Tuttavia, le classi risultano **sbilanciate**, rispecchiando la distribuzione osservativa reale, poiché il dataset è stato progettato proprio per simulare le osservazioni nel modo più fedele possibile.\n",
    "\n",
    "Di seguito sono elencati i nomi delle classi presenti nel dataset, accompagnati dalla rispettiva percentuale di distribuzione e dal metodo impiegato per la loro simulazione:\n",
    "1. **CONSTANT** 15% (Soraisam 2020), non presentano variazione, ma solo rumore statistico (quantificato come tipico dalle osservazioni già raccolte);\n",
    "2. **SPonly** 7% (Kiss 2006, Yang & Jiang 2011), sinusoidi regolari con periodo corto (+ rumore);\n",
    "3. **LSPonly** 29% (Percy & Sato 2009, Yang & Jiang 2011), sinusoidi regolari con periodo lungo (+ rumore);\n",
    "4. **SP+LSP** 14%, somma dei casi 2+3 (+ rumore);\n",
    "5. **IRREGULAR** 35% (Yang & Jiang 2011), simulate come somma dei casi 2+3, ma con periodo e ampiezza variabili rapidamente nel tempo (+ rumore).\n",
    "\n",
    "Le curve di luce sono state simulate su 912 giorni, pari a tre semetri osservativi e due di riposo, con valori distribuiti attorno a zero in quanto è stato sottratto il valore medio dell'intera curva. Nei semestri di riposo la mancanza di dati è stata flaggata ad un valore arbitrario pari a -99. Comunque, nei semestri osservativi vi sono anche dei valori pari a -99, in quanto è stata pure simulata una eventuale mancanza di dati nelle osservazioni (ad es. notti di maltempo, problemi strumentali,...).\n",
    "\n",
    "Nel mio progetto affronterò un **problema di classificazione multiclasse**, cercando di classificare le curve di luce in base alle loro caratteristiche. A questo scopo, utilizzerò come **features i singoli punti** che compongono ciascuna curva di luce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidConfigError",
     "evalue": "Invalid client secrets file ('Error opening file', 'client_secrets.json', 'No such file or directory', 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Mario Lauriano\\miniconda3\\envs\\machine_learning\\Lib\\site-packages\\oauth2client\\clientsecrets.py:121\u001b[0m, in \u001b[0;36m_loadfile\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m    122\u001b[0m         obj \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(fp)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'client_secrets.json'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidClientSecretsError\u001b[0m                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Mario Lauriano\\miniconda3\\envs\\machine_learning\\Lib\\site-packages\\pydrive\\auth.py:386\u001b[0m, in \u001b[0;36mGoogleAuth.LoadClientConfigFile\u001b[1;34m(self, client_config_file)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 386\u001b[0m   client_type, client_info \u001b[38;5;241m=\u001b[39m \u001b[43mclientsecrets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient_config_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m clientsecrets\u001b[38;5;241m.\u001b[39mInvalidClientSecretsError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32mc:\\Users\\Mario Lauriano\\miniconda3\\envs\\machine_learning\\Lib\\site-packages\\oauth2client\\clientsecrets.py:165\u001b[0m, in \u001b[0;36mloadfile\u001b[1;34m(filename, cache)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache:\n\u001b[1;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_loadfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m obj \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(filename, namespace\u001b[38;5;241m=\u001b[39m_SECRET_NAMESPACE)\n",
      "File \u001b[1;32mc:\\Users\\Mario Lauriano\\miniconda3\\envs\\machine_learning\\Lib\\site-packages\\oauth2client\\clientsecrets.py:124\u001b[0m, in \u001b[0;36m_loadfile\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 124\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidClientSecretsError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError opening file\u001b[39m\u001b[38;5;124m'\u001b[39m, exc\u001b[38;5;241m.\u001b[39mfilename,\n\u001b[0;32m    125\u001b[0m                                     exc\u001b[38;5;241m.\u001b[39mstrerror, exc\u001b[38;5;241m.\u001b[39merrno)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _validate_clientsecrets(obj)\n",
      "\u001b[1;31mInvalidClientSecretsError\u001b[0m: ('Error opening file', 'client_secrets.json', 'No such file or directory', 2)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidConfigError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Autenticazione e creazione dell'oggetto Google Drive\u001b[39;00m\n\u001b[0;32m      9\u001b[0m gauth \u001b[38;5;241m=\u001b[39m GoogleAuth()\n\u001b[1;32m---> 10\u001b[0m \u001b[43mgauth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLocalWebserverAuth\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Crea un server locale e autentica\u001b[39;00m\n\u001b[0;32m     11\u001b[0m drive \u001b[38;5;241m=\u001b[39m GoogleDrive(gauth)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Sostituisci con l'ID della tua cartella in Google Drive\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mario Lauriano\\miniconda3\\envs\\machine_learning\\Lib\\site-packages\\pydrive\\auth.py:113\u001b[0m, in \u001b[0;36mCheckAuth.<locals>._decorated\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLoadCredentials()\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflow \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetFlow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcredentials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m   code \u001b[38;5;241m=\u001b[39m decoratee(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mario Lauriano\\miniconda3\\envs\\machine_learning\\Lib\\site-packages\\pydrive\\auth.py:443\u001b[0m, in \u001b[0;36mGoogleAuth.GetFlow\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Gets Flow object from client configuration.\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \n\u001b[0;32m    439\u001b[0m \u001b[38;5;124;03m:raises: InvalidConfigError\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(config \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_config \\\n\u001b[0;32m    442\u001b[0m            \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCLIENT_CONFIGS_LIST):\n\u001b[1;32m--> 443\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoadClientConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    444\u001b[0m constructor_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mredirect_uri\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mredirect_uri\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauth_uri\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauth_uri\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_uri\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_uri\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    448\u001b[0m }\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrevoke_uri\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Mario Lauriano\\miniconda3\\envs\\machine_learning\\Lib\\site-packages\\pydrive\\auth.py:366\u001b[0m, in \u001b[0;36mGoogleAuth.LoadClientConfig\u001b[1;34m(self, backend)\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidConfigError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease specify client config backend\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 366\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoadClientConfigFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msettings\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    368\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLoadClientConfigSettings()\n",
      "File \u001b[1;32mc:\\Users\\Mario Lauriano\\miniconda3\\envs\\machine_learning\\Lib\\site-packages\\pydrive\\auth.py:388\u001b[0m, in \u001b[0;36mGoogleAuth.LoadClientConfigFile\u001b[1;34m(self, client_config_file)\u001b[0m\n\u001b[0;32m    386\u001b[0m   client_type, client_info \u001b[38;5;241m=\u001b[39m clientsecrets\u001b[38;5;241m.\u001b[39mloadfile(client_config_file)\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m clientsecrets\u001b[38;5;241m.\u001b[39mInvalidClientSecretsError \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m--> 388\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m InvalidConfigError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid client secrets file \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m error)\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m client_type \u001b[38;5;129;01min\u001b[39;00m (clientsecrets\u001b[38;5;241m.\u001b[39mTYPE_WEB,\n\u001b[0;32m    390\u001b[0m                        clientsecrets\u001b[38;5;241m.\u001b[39mTYPE_INSTALLED):\n\u001b[0;32m    391\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m InvalidConfigError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown client_type of client config file\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mInvalidConfigError\u001b[0m: Invalid client secrets file ('Error opening file', 'client_secrets.json', 'No such file or directory', 2)"
     ]
    }
   ],
   "source": [
    "#serve a collegare il notebook alla cartella drive creata per permettere la visualizzazione delle immagini\n",
    "# Importa le librerie necessarie\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Autenticazione e creazione dell'oggetto Google Drive\n",
    "gauth = GoogleAuth()\n",
    "gauth.LocalWebserverAuth()  # Crea un server locale e autentica\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "# Sostituisci con l'ID della tua cartella in Google Drive\n",
    "folder_id = 'https://drive.google.com/drive/folders/1SrkE7oIKBcVaYZWv0x9bGLKpoYB-7aL8?usp=drive_link'  # Trova l'ID dalla URL della tua cartella\n",
    "\n",
    "# Crea una cartella locale per scaricare le immagini\n",
    "local_folder = 'progetto_Lauriano'\n",
    "os.makedirs(local_folder, exist_ok=True)\n",
    "\n",
    "# Recupera i file nella cartella di Google Drive\n",
    "file_list = drive.ListFile({'q': f\"'{folder_id}' in parents and mimeType='image/jpeg' or mimeType='image/png'\"}).GetList()\n",
    "\n",
    "# Scarica tutte le immagini\n",
    "for file in file_list:\n",
    "    print(f'Downloading: {file[\"title\"]}')\n",
    "    downloaded_file = drive.CreateFile({'id': file['id']})\n",
    "    downloaded_file.GetContentFile(os.path.join(local_folder, file['title']))\n",
    "\n",
    "# Visualizza tutte le immagini scaricate\n",
    "for filename in os.listdir(local_folder):\n",
    "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "        img_path = os.path.join(local_folder, filename)\n",
    "        img = Image.open(img_path)\n",
    "        plt.imshow(img)\n",
    "        plt.title(filename)\n",
    "        plt.axis('off')  # Disattiva gli assi\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importo il dataset\n",
    "data = np.load('all_simulated_lc_v4.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chiavi presenti nel file NPZ: KeysView(NpzFile 'all_simulated_lc_v4.npz' with keys: sim_type, all_simulated_lc, bkg_sigma, good_fraction, seasonal_array...)\n",
      "'sim_type' ha la forma (10000,)\n",
      "'all_simulated_lc' ha la forma (10000, 912)\n",
      "'bkg_sigma' ha la forma (10000,)\n",
      "'good_fraction' ha la forma (10000,)\n",
      "'seasonal_array' ha la forma (912,)\n",
      "'period1' ha la forma (10000,)\n",
      "'amplitude1' ha la forma (10000,)\n",
      "'phase1' ha la forma (10000,)\n",
      "'period2' ha la forma (10000,)\n",
      "'amplitude2' ha la forma (10000,)\n",
      "'phase2' ha la forma (10000,)\n"
     ]
    }
   ],
   "source": [
    "#mostra le chiavi disponibili nel file\n",
    "#il file 'npz' permette di salvato molti array all'interno di un singolo elemento non compresso\n",
    "print(\"Chiavi presenti nel file NPZ:\", data.keys())\n",
    "\n",
    "# itera su ciascun array e ne stampa la forma (numero di righe e colonne)\n",
    "for key in data.keys():\n",
    "    array = data[key]\n",
    "    print(f\"'{key}' ha la forma {array.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sebbene il dataset presenti tante informazioni, le uniche che utilizzerò saranno le curve di luce contenute in 'all_simulated_lc'. Non farò uso di periodi, fasi, ampiezze ed errori perché sono proprio i valori utilizzati per costruire le curve! Pertanto, se dovessi classificare le curve da dati reali non sarebbero informazioni in mio possesso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLOT N-ESIMA CURVA DI LUCE ###\n",
    "all_simulated_lc = data['all_simulated_lc'] #estrae le curve di luce\n",
    "x=np.arange(len(all_simulated_lc[0])) #determina la lunghezza delle curve (912 giorni)\n",
    "n=0 #seleziona la n-esima curva di luce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea il grafico della n-esima curva\n",
    "plt.scatter(x, all_simulated_lc[n], c='blue', label='Osservazioni', s=4, alpha=0.7)\n",
    "plt.xlabel('Tempo [giorni]')\n",
    "plt.ylabel('Flusso normalizzato')\n",
    "plt.title(f'SP+LSP')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(-2,2)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'progetto\\\\lc_SP+LSP_n{n}.jpg',dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di seguito si riporta un esempio di ciascuna classe di curva di luce, per dare un'idea dei dati:\n",
    "\n",
    "<img src=\"progetto/lc_irregular_n0.jpg\" alt=\"irregular\" style=\"width: 300px;\"/>\n",
    "<img src=\"progetto/lc_SPonly_n2.jpg\" alt=\"SPonly\" style=\"width: 300px;\"/>\n",
    "<img src=\"progetto/lc_LSPonly_n3.jpg\" alt=\"LSPonly\" style=\"width: 300px;\"/>\n",
    "<img src=\"progetto/lc_SP+LSP_n5.jpg\" alt=\"SP+LSP_new\" style=\"width: 300px;\"/>\n",
    "<img src=\"progetto/lc_constant_n19.jpg\" alt=\"constant\" style=\"width: 300px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nella sezione successiva si interpolano le curve di luce per i dati mancanti all'interno delle finestre osservative, di modo da avere lo stesso numero di features fra tutte le curve e nelle stesse posizioni (i periodi dei semestri sono gli stessi per tutte le curve).\n",
    "Per l'interpolazione 1D sono state provate diverse strategie che saranno di seguito descritte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "from scipy.interpolate import Akima1DInterpolator\n",
    "\n",
    "#maschero i dati dei semestri di riposo così da lavorare soltanto sull'interpolazione \n",
    "# all'interno dei tre semetri osservativi\n",
    "seasonal_mask = data['seasonal_array'] == 1 #valore booleano: 1 semestre attivo, 0 semestre di riposo\n",
    "x_filt = x[seasonal_mask] #len(x_filt)=546\n",
    "\n",
    "#ciclo su tutte le curve di luce nel dataset\n",
    "masked_curves = []\n",
    "interpolated_curves = []\n",
    "i=0\n",
    "for i in range(len(all_simulated_lc)):\n",
    "    lc_simulation = all_simulated_lc[i] #estrae le curve\n",
    "    lc_masked = lc_simulation[seasonal_mask] #applica la maschera\n",
    "    masked_curves.append(lc_masked) #salva la i-esima curva mascherata\n",
    "\n",
    "    ### Interpolazione punti all'interno dei semestri osservativi ###\n",
    "    #all'interno dei semestri di riposo possono mancare dati per via di meteo avverso o malfunzionamenti strumentali\n",
    "    #per garantire che tutti gli array abbiano lo stesso numero di features (anche presenti nei medesimi giorni)\n",
    "    #si interpolano i dati\n",
    "    all_indices = np.arange(len(lc_masked)) #546\n",
    "    valid_indices = np.flatnonzero(lc_masked != -99) #x: posizioni in cui si registrano dati\n",
    "    valid_values = lc_masked[valid_indices]          #y: valori dei dati alle posizioni 'valid_indices'\n",
    "\n",
    "    # Funzione di interpolazione (quadratica)\n",
    "    interp_func = Akima1DInterpolator(valid_indices, valid_values, extrapolate=True)\n",
    "    #interp_func = interp1d(valid_indices, valid_values, kind='quadratic', bounds_error=False, fill_value=\"extrapolate\")\n",
    "    new_values = interp_func(all_indices) #applica l'interpolazione\n",
    "\n",
    "    # Sostituzione dei valori -99 con i nuovi valori\n",
    "    lc_inter = lc_masked.copy()  #nuovo array con i dati interpolati\n",
    "    lc_inter[lc_inter == -99] = new_values[lc_inter == -99]\n",
    "    interpolated_curves.append(lc_inter) #salva la curva interpolata\n",
    "\n",
    "# conversione interpolated_curves in un array numpy per operazioni future\n",
    "masked_curves = np.array(masked_curves)\n",
    "interpolated_curves = np.array(interpolated_curves)\n",
    "#np.save(\"interpolated_curves.npy\", interpolated_curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##crea un array per le curve finali con la dimensione completa di 912 giorni\n",
    "#quindi senza maschera sui semestri di riposo##\n",
    "##Strategia utile ai fini di una eventuale rete RNN##\n",
    "full_curves = np.copy(all_simulated_lc)\n",
    "\n",
    "i=0\n",
    "#ciclo per costruire le curve di luce finali\n",
    "for i in range(len(all_simulated_lc)):\n",
    "    # sostitue i valori nei giorni attivi con i valori interpolati\n",
    "    full_curves[i, seasonal_mask] = interpolated_curves[i]\n",
    "\n",
    "#salva le curve finali in un file CSV\n",
    "full_curves = full_curves[:, :-2]\n",
    "full_df = pd.DataFrame(full_curves)\n",
    "\n",
    "#estraggo le classi dal dataset simulato iniziale\n",
    "classes = [entry.split(',')[3].split(':')[0].strip() for entry in data['sim_type']]\n",
    "full_df['target'] = classes #inserisco la colonna con le classi nel file\n",
    "full_df.to_csv('interp_lc_5sem.csv', index=False) #salva un file esterno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Constant' 'Irregular' 'LSPonly' 'SP+LSP' 'SPonly']\n"
     ]
    }
   ],
   "source": [
    "#stampa delle classi presenti nel dataset (sono le stesse già anticipate sopra nel testo)\n",
    "print(np.unique(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot curva n-esima interpolata#\n",
    "plt.scatter(x_filt,interpolated_curves[n], label='LC interpolata', s=2, color='#ff7f0e')    #curva interpolata\n",
    "plt.scatter(x_filt,masked_curves[n], label='LC simulata', s=4, color='#0000CD', marker='o') #curva simulata\n",
    "\n",
    "type='Irregular'\n",
    "inter_type='inter1d quadratica'\n",
    "plt.title(f'{type} {inter_type}')\n",
    "plt.xlabel('Tempo [giorni]')\n",
    "plt.ylim(-2,2)\n",
    "plt.ylabel('Flusso normalizzato')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "#plt.savefig(f'progetto\\\\lc_{type}_n{n}_{inter_type}.jpg',dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descrizione delle due migliori strategie di interpolazione:\n",
    "\n",
    "**Akima1DInterpolator** [[doc](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.Akima1DInterpolator.html)]: interpolazione cubica a tratti, evitando eccessive variazioni fra i dati utilizzati per interpolare\n",
    "\n",
    "**inter1d** [[doc](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html)]: permette di interpolare una funzione 1d e di scegliere il grado con cui lo si vuole fare.\n",
    "\n",
    "Di seguito si riportano i plot di tutte e cinque le classi con le due differenti tipologie di interpolazione per osservare come la akima introduca meno irregolarità nell'interpolazione e per questo la si preferisce sull'altra.\n",
    "\n",
    "<img src=\"progetto/lc_irregular_n0_akima.jpg\" alt=\"irregular_akima\" style=\"width: 230px;\"/>\n",
    "<img src=\"progetto/lc_SPonly_n2_akima.jpg\" alt=\"SPonly_akima\" style=\"width: 230px;\"/>\n",
    "<img src=\"progetto/lc_LSPonly_n3_akima.jpg\" alt=\"LSPonly_akima\" style=\"width: 230px;\"/>\n",
    "<img src=\"progetto/lc_SP+LSP_n5_akima.jpg\" alt=\"SP+LSP_akima\" style=\"width: 230px;\"/>\n",
    "<img src=\"progetto/lc_constant_n19_akima.jpg\" alt=\"constant_akima\" style=\"width: 230px;\"/>\n",
    "\n",
    "<img src=\"progetto/lc_irregular_n0_inter1d quadratica.jpg\" alt=\"irregular_inter1d\" style=\"width: 230px;\"/>\n",
    "<img src=\"progetto/lc_SPonly_n2_inter1d quadratica.jpg\" alt=\"SPonly_inter1d\" style=\"width: 230px;\"/>\n",
    "<img src=\"progetto/lc_LSPonly_n3_inter1d quadratica.jpg\" alt=\"LSPonly_inter1d\" style=\"width: 230px;\"/>\n",
    "<img src=\"progetto/lc_SP+LSP_n5_inter1d quadratica.jpg\" alt=\"SP+LSP_inter1d\" style=\"width: 230px;\"/>\n",
    "<img src=\"progetto/lc_constant_n19_inter1d quadratica.jpg\" alt=\"constant_inter1d\" style=\"width: 230px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come già anticipato, ogni punto della cura di luce rappresenterà una feature; infatti, sebbene ci siano sicuramente altre possibilità, come quella di fittare le curve ed estrapolare informazioni di carattere fisico come periodo, fase e ampiezza, l'approccio seguito permette di mantenere il problema di classificazioni ad un livello di astrazione maggiore, rendendo più interessante la ricerca del corretto algoritmo di intelligenza artificiale in grado di individuare pattern nei dati.\n",
    "\n",
    "Tuttavia, dato che un alto numero di features rende computazionalmente più impegnativa l'analisi (il mio PC presenta 6 core fisici i7 9H) e con certi modelli si rischia anche di incorrere in overfitting, ho provato in un primo momento a rebinnare le curve di luce per diminuire il numero di features di un fattore 'num_points', cioè il numero di punti di cui si intendeva calcolare la media per generare un unico punto posizionato al centro dell'intervallo composto da 'num_points' punti della curva di luce.\n",
    "\n",
    "Di seguito, commentato, riporto l'algoritmo scritto, sebbene non sia poi stato utilizzato perché si perdeva informazione sui massimi e minimi locali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "### REBINNING ###\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Assuming active_blocks is already defined and contains tuples of (start, end) for each active block\n",
    "# Function to check if a point is within any active block\n",
    "def is_within_active_blocks(point, active_blocks):\n",
    "    return any(start <= point <= end for start, end in active_blocks)\n",
    "\n",
    "### FUNZIONE DI REBINNING\n",
    "def rebin_curve(lc_inter, x_filt, num_points):\n",
    "    # Applica la maschera selezionando solo i punti interpolati e filtrando valori -99\n",
    "    valid_indices = np.where(lc_inter != -99)[0]\n",
    "    valid_x = x_filt[valid_indices]\n",
    "    valid_y = lc_inter[valid_indices]\n",
    "    \n",
    "    # Array per salvare i dati rebinnati\n",
    "    x_new = []\n",
    "    y_new = []\n",
    "\n",
    "    for i in np.arange(0, len(valid_x) - num_points + 1, num_points):\n",
    "        # Calcola la media degli m consecutivi punti in x\n",
    "        xmid = np.mean(valid_x[i:i + num_points])\n",
    "\n",
    "        # controlla se xmid è all'interno del blocco attivo (se num_points non è un divisore della len(active_block) allora un punto cadrà nel semestre di riposo)\n",
    "        if is_within_active_blocks(xmid, active_blocks):\n",
    "            x_new.append(xmid)\n",
    "\n",
    "            # Calcola la media di m consecutivi punti in y\n",
    "            ymid = np.mean(valid_y[i:i + num_points])\n",
    "            y_new.append(ymid)\n",
    "    \n",
    "    return np.array(x_new), np.array(y_new)\n",
    "\n",
    "num_points = 2  # numero di punti rebinnati\n",
    "results = Parallel(n_jobs=-1)(delayed(rebin_curve)(interpolated_curves[i], x_filt, num_points) for i in range(len(interpolated_curves)))\n",
    "rebin_lc_x, rebin_lc_y = zip(*results) #separo i risultati rebinned in due array x e y\n",
    "\n",
    "# plot\n",
    "plt.scatter(rebin_lc_x[n], rebin_lc_y[n], s=2, color='#ff7f0e', label=f'Interpolati bin{num_points}')\n",
    "plt.scatter(x_filt, masked_curves[n], s=3.5, color='#0000CD', label='Osservazioni')\n",
    "plt.ylim(-1.2, 1.2) #serve per visualizzare correttamente le osservazioni originali\n",
    "plt.xlabel('Tempo [giorni]')\n",
    "plt.ylabel('Flusso normalizzato')\n",
    "plt.title(f'Curva di luce {n + 1} - Rebinned con {num_points}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest e XGBoost\n",
    "Un primo approccio alla classificazione è stato compiuto impiegando i modelli *Random Forest* e *XGBoost*, di cui qui viene fornita una breve descizione.  \n",
    "\n",
    "**Random Forest** [[doc](https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html)] è un insieme di alberi decisionali, ognuno con specialità leggermente diverse, e la previsione finale è interpretabile come una decisione collettiva, presa dopo aver considerato il contributo di tutti gli alberi.\n",
    "\n",
    "Di seguito uno schema rappresentativo:\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*M6ZBzDoQH1Vs3LRCpT5v2Q.png\" alt=\"Alt text\" width=\"500\"/>\n",
    "\n",
    "\n",
    "\n",
    "**XGBoost (eXtreme Gradient Boosting)**: il concetto fondamentale di XGBoost, come per altri metodi di boosting, è quello di aggiungere nuovi modelli all'ensemble in modo sequenziale. Tuttavia, a differenza di Random Forest, dove gli alberi vengono fatti crescere in parallelo, i metodi di boosting addestrano i modelli in serie uno dopo l'altro, con ogni nuovo albero che contribuisce a correggere gli errori commessi dall'albero precedente.\n",
    "\n",
    "#### *Osservazioni importanti !*\n",
    "*Standardizzazione/normalizzazione*: per questi due modelli non è necessario standardizzare o normalizzare le feature, in quanto gli alberi decisionali (sui quali si basa pure XGBoost) non vengono influenzati dalle scale delle variabili;\n",
    "\n",
    "*Etichette*: Random Forest accetta categorie (quindi parole) come etichette, mentre XGBoost richiede etichette scalari. Pertanto, per quest'ultimo si utilizza la funzione *LabelEncoder* per convertirle (N.B. funzione diversa da *OrdinalEncoder*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "'''\n",
    "# Funzione per applicare l'errore poissoniano su una curva di luce\n",
    "# siccome le curve sono normalizzate a 0, anche un poissoniano introdurrebbe un errore troppo grande\n",
    "#ad esempio un punto y=0.5 scatterato in [-poisson_noise, poisson_noise] assumerebbe valori da [-0.2, 1.2]\n",
    "\n",
    "def poisson_noise(curve):\n",
    "    poisson_noise = np.sqrt(np.abs(curve)) #calcola l'errore\n",
    "    curve_with_noise = curve + np.random.uniform(-poisson_noise, poisson_noise) #applica l'errore\n",
    "    return curve_with_noise\n",
    "'''\n",
    "\n",
    "### CREO TRE DATAFRAME DI PANDAS, ciascuno contenente rispettivamente le classi sbilanciate, upsampled e downsampled\n",
    "#per evitare il problema del data-leakage\n",
    "i=0\n",
    "df = pd.DataFrame(interpolated_curves, columns=[f'feature_{i}' for i in range(np.array(interpolated_curves).shape[1])])\n",
    "df_upsampling = pd.DataFrame(interpolated_curves, columns=[f'feature_{i}' for i in range(np.array(interpolated_curves).shape[1])])\n",
    "df_downsampling = pd.DataFrame(interpolated_curves, columns=[f'feature_{i}' for i in range(np.array(interpolated_curves).shape[1])])\n",
    "df['target'] = df_upsampling['target'] = df_downsampling['target'] = classes\n",
    "\n",
    "#estraggo features e classi per ciascuno dei tre dataset\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "\n",
    "X_up = df_upsampling.drop(columns=['target'])\n",
    "y_up = df_upsampling['target']\n",
    "#X_up_poisson = np.apply_along_axis(poisson_noise, axis=1, arr=X_up_res) #per applicare il rumore poissoniano\n",
    "\n",
    "X_down = df_downsampling.drop(columns=['target'])\n",
    "y_down = df_downsampling['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##qui si separa il 20% dei training dataset a formare i test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split train e test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_up, X_test_up, y_train_up, y_test_up = train_test_split(X_up, y_up, test_size=0.2, random_state=42)\n",
    "X_train_down, X_test_down, y_train_down, y_test_down = train_test_split(X_down, y_down, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dal momento che le classi all'interno dei dati risultano sbilanciate, per evitare di introdurre un bias nell'apprendimento e che i modelli finiscano per favorire le classi più rappresentate si intende adesso bilanciare le classi del train dataset. Infatti, è importante che i mdelli apprendano da dati bilanciati, ma possono essere testati su un dataset sbilanciato. Quest'ultimo punto è assai importante per due motivi:\n",
    "\n",
    "- il test non deve mai essere stato 'visto' ed esercitandovi data augmentation si rischia oltre che il data leackage anche di avere degli esempi uguali a quelli contenuti nel training se si utilizza, ad esempio, Randomoversampler;\n",
    "\n",
    "- nel caso specifico di questo problema di classificazione, testando l'algoritmo su un test sbilanciato si tiene fede al reale sbilanciamento delle osservazioni.\n",
    "\n",
    "\n",
    "Le funzioni di data augmentation messe a disposizione da scikit-learn sono le seguenti:\n",
    "\n",
    "*Upsampling* [[random](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html)] [[SMOTE](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html)]: è una strategia di 'data augmentation' per aumentare il numero di esempi delle classi meno rappresentate, normalizzando al valore della classe maggiore;\n",
    "\n",
    "*Downsamplign* [[random](https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html)]: normalizza le popolazioni di tutte le classi a quella meno rappresentata, scegliendo randomicamente quali dati mantenere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA AUGMENTATION ###\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "#Esistono due funzioni per fare data augmentation 'SMOTE' e 'RandomOverSampler'\n",
    "# che sono state entrambe testate ed i cui risultati erano confrontabili.\n",
    "#SMOTE, come altri algoritmi statistici, lavora con dati numerici e richiede sia insiemi \n",
    "#di caratteristiche che insiemi di etichette.\n",
    "'''\n",
    "ros = RandomOverSampler(sampling_strategy='not majority', random_state=42)\n",
    "X_train_up_res, y_train_up_res = ros.fit_resample(X_train_up, y_train_up)\n",
    "'''\n",
    "smote = SMOTE(sampling_strategy='not majority', random_state=42)\n",
    "X_train_up_res, y_train_up_res = smote.fit_resample(X_train_up, y_train_up)\n",
    "#X_up_poisson = np.apply_along_axis(poisson_noise, axis=1, arr=X_up_res)\n",
    "\n",
    "\n",
    "## DOWNSAPLING ##\n",
    "#è un'altra strategia, utile a non aggiungere rumore che potrebbe creare distorsioni nei dati\n",
    "rus = RandomUnderSampler(sampling_strategy='not minority', random_state=42)\n",
    "X_train_down_res, y_train_down_res = rus.fit_resample(X_train_down, y_train_down)\n",
    "\n",
    "## Analisi della distribuzione delle classi ##\n",
    "def plot_class_distribution(y, title,filename):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.countplot(x=y,color='orange',width=0.6)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Classi\")\n",
    "    plt.ylabel(\"Frequenza\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, format='jpg', dpi=150)\n",
    "    plt.close()  #chiude la figura per liberare memoria\n",
    "\n",
    "#plot della distribuzione delle classi originarie e di quelle down/up-sampled\n",
    "plot_class_distribution(y, \"Distribuzione delle Classi - Sbilanciate\", \"progetto//train_unbal.jpg\")\n",
    "plot_class_distribution(y_train_up_res, \"Distribuzione delle Classi - Bilanciata upsampling\", \"progetto//train_upres.jpg\")\n",
    "plot_class_distribution(y_train_down_res, \"Distribuzione delle Classi - Bilanciata downsapling\", \"progetto//train_downres.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Popolazione delle classi del training dataset nel caso originale e nei casi sia di upsampling che di downsampling.\n",
    "\n",
    "<img src=\"progetto/train_unbal.jpg\" alt=\"y\" style=\"width: 300px;\"/>\n",
    "<img src=\"progetto/train_upres.jpg\" alt=\"y_train_up_res\" style=\"width: 300px;\"/>\n",
    "<img src=\"progetto/train_downres.jpg\" alt=\"y_train_down_res\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest e XGBoost\n",
    "\n",
    "Random forest, come del resto tutti i modelli di ML, si basa su degli iperparametri, il cui valore può significativamente influenzare i risultati del modello. Poiché non si conoscono a priori i migliori valori di questi parametri e ricercarli manualmente sarebbe alquanto lungo e fortunoso, si preferisce scegliere diversi valori presumibilmente corretti ed esplorarli nelle varie combinazioni per mezzo della funzione *[GridSearchCV](https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.GridSearchCV.html)*.\n",
    "\n",
    "*Attenzione!*\n",
    "Nel codice è commentata la gridia inizialmente utilizzata, ma sotto si riporta una griglia con i soli valori best osservati, di modo da evitare all'utente che compilerà una lunga attesa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Migliori iperparametri Random Forest (sbilanciato): {'bootstrap': False, 'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Random Forest (sbilanciato) - Report di classificazione:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Constant       0.77      0.96      0.85       271\n",
      "   Irregular       0.95      1.00      0.98       716\n",
      "     LSPonly       0.87      0.88      0.88       592\n",
      "      SP+LSP       0.67      0.63      0.65       274\n",
      "      SPonly       0.47      0.19      0.27       147\n",
      "\n",
      "    accuracy                           0.85      2000\n",
      "   macro avg       0.75      0.73      0.72      2000\n",
      "weighted avg       0.83      0.85      0.83      2000\n",
      "\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Migliori iperparametri Random Forest (bilanciato upsampling): {'bootstrap': False, 'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Random Forest (bilanciato upsampling) - Report di classificazione:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Constant       0.76      0.97      0.85       271\n",
      "   Irregular       0.96      1.00      0.98       716\n",
      "     LSPonly       0.91      0.87      0.89       592\n",
      "      SP+LSP       0.69      0.54      0.61       274\n",
      "      SPonly       0.47      0.39      0.43       147\n",
      "\n",
      "    accuracy                           0.85      2000\n",
      "   macro avg       0.76      0.75      0.75      2000\n",
      "weighted avg       0.84      0.85      0.84      2000\n",
      "\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Migliori iperparametri Random Forest (bilanciato downsampling): {'bootstrap': False, 'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Random Forest (bilanciato downsampling) - Report di classificazione:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Constant       0.66      0.99      0.79       271\n",
      "   Irregular       0.95      1.00      0.98       716\n",
      "     LSPonly       0.91      0.79      0.85       592\n",
      "      SP+LSP       0.70      0.42      0.52       274\n",
      "      SPonly       0.44      0.50      0.47       147\n",
      "\n",
      "    accuracy                           0.82      2000\n",
      "   macro avg       0.73      0.74      0.72      2000\n",
      "weighted avg       0.83      0.82      0.81      2000\n",
      "\n",
      "Random Forest (sbilanciato) - Accuratezza: 0.848\n",
      "Random Forest bilanciato upsampling - Accuratezza: 0.85\n",
      "Random Forest bilanciato downsampling - Accuratezza: 0.8195\n"
     ]
    }
   ],
   "source": [
    "### RANDOM FOREST ###\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# ricerca iperparametri per Random Forest\n",
    "'''\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50,100,200],\n",
    "    'max_depth': [10,20,30],\n",
    "    'min_samples_split': [2,5,7],\n",
    "    'min_samples_leaf': [1,2],\n",
    "    'bootstrap': [True,False]\n",
    "}\n",
    "'''\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [200],\n",
    "    'max_depth': [20],\n",
    "    'min_samples_split': [5],\n",
    "    'min_samples_leaf': [2],\n",
    "    'bootstrap': [False]\n",
    "}\n",
    "\n",
    "# GridSearchCV per Random Forest (dati sbilanciati)\n",
    "rf_model = RandomForestClassifier(random_state=42, n_jobs=-1) #n_jobs=-1 indica di parallelizzare su tutti i core fisici disponibili\n",
    "rf_grid_search = GridSearchCV(estimator=rf_model, param_grid=rf_param_grid,\n",
    "                               cv=3, verbose=1, n_jobs=-1)\n",
    "rf_grid_search.fit(X_train, y_train)  # addestramento con dati sbilanciati\n",
    "print(\"Migliori iperparametri Random Forest (sbilanciato):\", rf_grid_search.best_params_)\n",
    "\n",
    "# Previsione e report per Random Forest (dati sbilanciati)\n",
    "rf_best_model = rf_grid_search.best_estimator_\n",
    "y_pred_rf = rf_best_model.predict(X_test)\n",
    "print(\"Random Forest (sbilanciato) - Report di classificazione:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "#---------------#\n",
    "\n",
    "# GridSearchCV per Random Forest (bilanciato upsampling)\n",
    "rf_grid_search_up = GridSearchCV(estimator=rf_model, param_grid=rf_param_grid,\n",
    "                                     cv=3, verbose=1, n_jobs=-1)\n",
    "rf_grid_search_up.fit(X_train_up_res, y_train_up_res)\n",
    "print(\"Migliori iperparametri Random Forest (bilanciato upsampling):\", rf_grid_search_up.best_params_)\n",
    "\n",
    "# Previsione e report per Random Forest (bilanciato upsampling)\n",
    "rf_best_model_up = rf_grid_search_up.best_estimator_\n",
    "y_pred_rf_up = rf_best_model_up.predict(X_test_up)\n",
    "print(\"Random Forest (bilanciato upsampling) - Report di classificazione:\")\n",
    "print(classification_report(y_test_up, y_pred_rf_up))\n",
    "\n",
    "#---------------#\n",
    "\n",
    "# GridSearchCV per Random Forest (bilanciato downsampling)\n",
    "rf_grid_search_down = GridSearchCV(estimator=rf_model, param_grid=rf_param_grid,\n",
    "                                     cv=3, verbose=1, n_jobs=-1)\n",
    "rf_grid_search_down.fit(X_train_down_res, y_train_down_res)\n",
    "print(\"Migliori iperparametri Random Forest (bilanciato downsampling):\", rf_grid_search_down.best_params_)\n",
    "\n",
    "# Previsione e report per Random Forest (bilanciati downsampling)\n",
    "rf_best_model_down = rf_grid_search_down.best_estimator_\n",
    "y_pred_rf_down = rf_best_model_down.predict(X_test_down)\n",
    "print(\"Random Forest (bilanciato downsampling) - Report di classificazione:\")\n",
    "print(classification_report(y_test_down, y_pred_rf_down))\n",
    "\n",
    "# accuratezza modelli ottimizzati\n",
    "print(\"Random Forest (sbilanciato) - Accuratezza:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Random Forest bilanciato upsampling - Accuratezza:\", accuracy_score(y_test_up, y_pred_rf_up))\n",
    "print(\"Random Forest bilanciato downsampling - Accuratezza:\", accuracy_score(y_test_down, y_pred_rf_down))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A parte i punteggi dettagliati che si osservano dopo la compilazione della cella superiore, si preferisce generare una matrice di confusione per osservare effettivamente quanti dei dati risultano correttamente predetti. Di tale matrice si definisce qui sotto una funzione che verrà richiamata ad ogni modello e per la quale l'utente avrà la possibilità di modificare titolo del plot e nome del file ad ogni nuova compilazione. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFINIZIONE FUNZIONE MATRICE DI CONFUSIONE ##\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "title='Matrici di confusione Random Forest con interpolazione akima e upsampling SMOTE'\n",
    "filename='RF_akima_smote'\n",
    "\n",
    "def conf_matrix(cm1, cm2, cm3, title1, title2, title3, class_labels):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    fig.suptitle(f'{title}', fontsize=16, y=0.97)  # Aggiunge titolo generale\n",
    "\n",
    "    # prima matrice di confusione (dati sbilanciati)\n",
    "    sns.heatmap(cm1, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "                xticklabels=class_labels, yticklabels=class_labels)\n",
    "    axes[0].set_title(title1)\n",
    "    axes[0].set_xlabel('Predicted')\n",
    "    axes[0].set_ylabel('Actual')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # seconda matrice (dati upsampled)\n",
    "    sns.heatmap(cm2, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
    "                xticklabels=class_labels, yticklabels=class_labels)\n",
    "    axes[1].set_title(title2)\n",
    "    axes[1].set_xlabel('Predicted')\n",
    "    axes[1].set_ylabel('Actual')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # terza matrice (dati downsampled)\n",
    "    sns.heatmap(cm3, annot=True, fmt='d', cmap='Blues', ax=axes[2],\n",
    "                xticklabels=class_labels, yticklabels=class_labels)\n",
    "    axes[2].set_title(title3)\n",
    "    axes[2].set_xlabel('Predicted')\n",
    "    axes[2].set_ylabel('Actual')\n",
    "    axes[2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'progetto//cm_{filename}.jpg', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "class_labels = sorted(y.unique()) #etichette delle classi\n",
    "\n",
    "##plot matrici di confusione Random Forest\n",
    "# reazione delle matrici\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "cm_rf_up = confusion_matrix(y_test_up, y_pred_rf_up)\n",
    "cm_rf_down = confusion_matrix(y_test_down, y_pred_rf_down)\n",
    "\n",
    "#plot\n",
    "conf_matrix(cm_rf, cm_rf_up, cm_rf_down,\n",
    "                \"train sbilanciato\", \"train upsampled\", \"train downsampled\", class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adesso si definisce una matrice per la stampa delle *curve ROC (Receiver Operating Characteristic)*, cioè un grafico che mostra le prestazioni di un modello di classificazione per diversi valori di soglia di decisione. La curva rappresenta la relazione tra tasso di veri positivi (TPR) e tasso di falsi positivi (FPR), la proporzione di campioni negativi classificati erroneamente come positivi. Di ciascuna curva si calcola anche\n",
    "l'*AUC (Area Under the Curve)* che riassume le prestazioni del modello; infatti, AUC pari ad uno indica un modello perfetto, che distingue correttamente tutte le classi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNZIONE GRAFICO CURVA ROC E CALCOLO AUC ##\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "filename='curve_ROC_RF_akima'\n",
    "title_plot= 'Confronto Curve ROC Medie - Random Forest'\n",
    "\n",
    "#funzione per calcolare e tracciare le curve ROC medie\n",
    "def plot_average_roc_curve(y_tests, y_pred_probas, titles, class_labels):\n",
    "    n_classes = len(class_labels)  # numero di classi nel dataset\n",
    "    plt.figure(figsize=(10, 8))  # crea una figura per plottare le curve ROC\n",
    "\n",
    "    ##dati per la curva ROC media: prende in considerazione l'accuratezza media di tutte le classi##\n",
    "    mean_fpr = np.linspace(0, 1, 100)  # creazione di un array per FPR\n",
    "    \n",
    "    for y_test, y_pred_proba, title in zip(y_tests, y_pred_probas, titles):\n",
    "        #binarizza le etichette per ROC multi-classe\n",
    "        y_test_bin = label_binarize(y_test, classes=class_labels)  # conversione etichette in formato binario\n",
    "        \n",
    "        tprs = []  # lista per memorizzare TPR\n",
    "        aucs = []  # lista per memorizzare le AUC\n",
    "        \n",
    "        for i in range(n_classes):\n",
    "            fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])  # calcola i valori FPR e TPR per la classe i\n",
    "            roc_auc = auc(fpr, tpr)  # calcola l'AUC per la curva ROC della classe i\n",
    "            \n",
    "            #interpolazione per ottenere una curva liscia\n",
    "            tpr_interp = np.interp(mean_fpr, fpr, tpr)  # interpolazione dei TPR\n",
    "            tpr_interp[0] = 0.0\n",
    "            tprs.append(tpr_interp)  # aggiunge la curva TPR interpolata alla lista\n",
    "            aucs.append(roc_auc)     # aggiunge AUC alla lista\n",
    "        \n",
    "        # media delle TPR e AUC\n",
    "        mean_tpr = np.mean(tprs, axis=0)   #calcola la media delle TPR per tutte le classi\n",
    "        mean_auc = auc(mean_fpr, mean_tpr) #calcola l'AUC media\n",
    "\n",
    "        # curva ROC media\n",
    "        plt.plot(mean_fpr, mean_tpr, label=f\"{title} (AUC = {mean_auc:.4f})\")  # disegna la curva ROC media con la legenda\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")  # disegna una linea diagonale di riferimento (random chance)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f'{title_plot}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(f'progetto//{filename}.jpg', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "class_labels = sorted(y.unique())  # etichette delle classi\n",
    "\n",
    "#probabilità predette\n",
    "y_pred_proba_rf = rf_best_model.predict_proba(X_test)  # probabilità previste per il modello Random Forest con dati sbilanciati\n",
    "y_pred_proba_rf_up = rf_best_model_up.predict_proba(X_test_up)  # dati upsampled\n",
    "y_pred_proba_rf_down = rf_best_model_down.predict_proba(X_test_down)  # dati downsampled\n",
    "\n",
    "# plot la curva ROC media\n",
    "plot_average_roc_curve(\n",
    "    y_tests=[y_test, y_test_up, y_test_down],  # dati di test per ciascun modello\n",
    "    y_pred_probas=[y_pred_proba_rf, y_pred_proba_rf_up, y_pred_proba_rf_down],  #probabilità previste per ciascun modello\n",
    "    titles=[\"Dati Sbilanciati\", \"Dati Upsampled\", \"Dati Downsampled\"], \n",
    "    class_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Migliori iperparametri XGBoost (sbilanciato): {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 20, 'n_estimators': 200, 'subsample': 1.0}\n",
      "XGBoost (sbilanciato) - Report di classificazione:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Constant       0.81      0.94      0.87       271\n",
      "   Irregular       0.95      1.00      0.98       716\n",
      "     LSPonly       0.89      0.91      0.90       592\n",
      "      SP+LSP       0.65      0.60      0.62       274\n",
      "      SPonly       0.42      0.21      0.28       147\n",
      "\n",
      "    accuracy                           0.85      2000\n",
      "   macro avg       0.74      0.73      0.73      2000\n",
      "weighted avg       0.83      0.85      0.84      2000\n",
      "\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Migliori iperparametri XGBoost (upsampling): {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 20, 'n_estimators': 200, 'subsample': 1.0}\n",
      "XGBoost bilanciato upsampling - Report di classificazione:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Constant       0.78      0.94      0.85       271\n",
      "   Irregular       0.96      1.00      0.98       716\n",
      "     LSPonly       0.91      0.90      0.90       592\n",
      "      SP+LSP       0.66      0.57      0.61       274\n",
      "      SPonly       0.45      0.33      0.38       147\n",
      "\n",
      "    accuracy                           0.85      2000\n",
      "   macro avg       0.75      0.75      0.75      2000\n",
      "weighted avg       0.84      0.85      0.84      2000\n",
      "\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Migliori iperparametri XGBoost (downsapling): {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 20, 'n_estimators': 200, 'subsample': 1.0}\n",
      "XGBoost bilanciato downsampling - Report di classificazione:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Constant       0.70      0.99      0.82       271\n",
      "   Irregular       0.96      0.99      0.97       716\n",
      "     LSPonly       0.92      0.80      0.86       592\n",
      "      SP+LSP       0.58      0.36      0.45       274\n",
      "      SPonly       0.38      0.50      0.43       147\n",
      "\n",
      "    accuracy                           0.81      2000\n",
      "   macro avg       0.71      0.73      0.71      2000\n",
      "weighted avg       0.82      0.81      0.81      2000\n",
      "\n",
      "XGBoost (sbilanciato) - Accuratezza: 0.8525\n",
      "XGBoost bilanciato upsampling SMOTE - Accuratezza: 0.852\n",
      "XGBoost bilanciato downsampling - Accuratezza: 0.812\n"
     ]
    }
   ],
   "source": [
    "### XGBoost ###\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "label_encoder = LabelEncoder() # codifica delle etichette scalari per XGBoost\n",
    "\n",
    "#sbilanciati\n",
    "y_train_enc = label_encoder.fit_transform(y_train)\n",
    "y_test_enc = label_encoder.transform(y_test)\n",
    "\n",
    "#bilanciati upsampling\n",
    "y_train_up_enc = label_encoder.fit_transform(y_train_up_res)\n",
    "y_test_up_enc = label_encoder.fit_transform(y_test_up)\n",
    "\n",
    "#bilanciati downsampling\n",
    "y_train_down_enc = label_encoder.transform(y_train_down_res)\n",
    "y_test_down_enc = label_encoder.transform(y_test_down)\n",
    "\n",
    "# ricerca iperparametri per XGBoost\n",
    "'''\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [50,100,200],\n",
    "    'max_depth': [3,6,10],\n",
    "    'learning_rate': [0.01,0.1,0.2],\n",
    "    'subsample': [0.8,1.0],\n",
    "    'colsample_bytree': [0.8,1.0]\n",
    "}\n",
    "'''\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [200],\n",
    "    'max_depth': [20],\n",
    "    'learning_rate': [0.2],\n",
    "    'subsample': [1.0],\n",
    "    'colsample_bytree': [0.8]\n",
    "}\n",
    "\n",
    "# GridSearchCV (dati sbilanciati)\n",
    "xgb_model = XGBClassifier(eval_metric='mlogloss', n_jobs=-1)\n",
    "# mlogloss: calcola la perdita logaritmica per problemi di classificazione multi-classe, \n",
    "# misurando quanto bene il modello predice le probabilità delle classi corrette\n",
    "xgb_grid_search = GridSearchCV(estimator=xgb_model, param_grid=xgb_param_grid,\n",
    "                                cv=3, verbose=1, n_jobs=-1)\n",
    "xgb_grid_search.fit(X_train, y_train_enc)  # addestramento con dati sbilanciati\n",
    "print(\"Migliori iperparametri XGBoost (sbilanciato):\", xgb_grid_search.best_params_)\n",
    "\n",
    "# Previsione e report (dati sbilanciati)\n",
    "xgb_best_model = xgb_grid_search.best_estimator_\n",
    "y_pred_xgb = xgb_best_model.predict(X_test)\n",
    "y_pred_xgb = label_encoder.inverse_transform(y_pred_xgb)  # decodifica delle etichette\n",
    "print(\"XGBoost (sbilanciato) - Report di classificazione:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "#---------------#\n",
    "\n",
    "# GridSearchCV (upsampling)\n",
    "xgb_grid_search_up = GridSearchCV(estimator=xgb_model, param_grid=xgb_param_grid,\n",
    "                                      cv=3, verbose=1, n_jobs=-1)\n",
    "xgb_grid_search_up.fit(X_train_up_res, y_train_up_enc) #dati aumentati\n",
    "print(\"Migliori iperparametri XGBoost (upsampling):\", xgb_grid_search_up.best_params_)\n",
    "\n",
    "# Previsione e report (upsampling)\n",
    "xgb_best_model_up = xgb_grid_search_up.best_estimator_\n",
    "y_pred_xgb_up = xgb_best_model_up.predict(X_test_up)\n",
    "y_pred_xgb_up = label_encoder.inverse_transform(y_pred_xgb_up)\n",
    "print(\"XGBoost bilanciato upsampling - Report di classificazione:\")\n",
    "print(classification_report(y_test_up, y_pred_xgb_up))\n",
    "\n",
    "#---------------#\n",
    "\n",
    "# GridSearchCV (downsampling)\n",
    "xgb_grid_search_down = GridSearchCV(estimator=xgb_model, param_grid=xgb_param_grid,\n",
    "                                      cv=3, verbose=1, n_jobs=-1)\n",
    "xgb_grid_search_down.fit(X_train_down_res, y_train_down_enc)\n",
    "print(\"Migliori iperparametri XGBoost (downsapling):\", xgb_grid_search_down.best_params_)\n",
    "\n",
    "# previsione e report (downsapling)\n",
    "xgb_best_model_down = xgb_grid_search_down.best_estimator_\n",
    "y_pred_xgb_down = xgb_best_model_down.predict(X_test_down)\n",
    "y_pred_xgb_down = label_encoder.inverse_transform(y_pred_xgb_down)\n",
    "print(\"XGBoost bilanciato downsampling - Report di classificazione:\")\n",
    "print(classification_report(y_test_down, y_pred_xgb_down))\n",
    "\n",
    "#accuratezza modelli ottimizzati\n",
    "print(\"XGBoost (sbilanciato) - Accuratezza:\", accuracy_score(y_test, y_pred_xgb))\n",
    "print(\"XGBoost bilanciato upsampling SMOTE - Accuratezza:\", accuracy_score(y_test_up, y_pred_xgb_up))\n",
    "print(\"XGBoost bilanciato downsampling - Accuratezza:\", accuracy_score(y_test_down, y_pred_xgb_down))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#richiamo la funzione e stampo la matrice di confusione XGBoost\n",
    "\n",
    "title='Matrici di confusione XGBoost con interpolazione akima e upsampling SMOTE'\n",
    "filename='XGB_akima_smote'\n",
    "\n",
    "cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "cm_xgb_up = confusion_matrix(y_test_up, y_pred_xgb_up)\n",
    "cm_xgb_down = confusion_matrix(y_test_down, y_pred_xgb_down)\n",
    "\n",
    "conf_matrix(cm_xgb, cm_xgb_up, cm_xgb_down,\n",
    "        \"train sbilanciato\", \"train upsampled\", \"train downsampled\", class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di seguito sono riportate le matrici di confusione di *Random Forest* e *XGBoost*. Si osserva che i risultati sono confrontabili tra loro; inoltre, è ragionevole desumere la presenza di overfitting sulla classe 'irregular' che oltre ad essere quella più rappresentata nel dataset di partenza, manifesta la maggiore etereogeneità. È anche evidente come le classi più confuse sono *SP+LSP* e *SPonly*, risultato attenbile alla luce del fatto che entrambe presentano un pattern molto evidente nel breve periodo, con la differenza che la prima classe sarebbe caratterizzata pure da una periodicità maggiore (LSP) che però, evidentemente, non viene colta da questi algoritmi.\n",
    "Si osserva anche una leggera confusione tra le classi *constant* e *LSPonly*: pure questa prevedibile in quanto il pattern sul lungo periodo se non colto può essere ben confuso con un pattern costante.\n",
    "\n",
    "<img src=\"progetto/cm_RF_akima_smote.jpg\" alt=\"y\" style=\"width: 1000px;\"/>\n",
    "<img src=\"progetto/cm_XGB_akima_smote.jpg\" alt=\"y\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## richiamo la funzione e stampo le curve ROC per XGBoost ##\n",
    "\n",
    "filename='curve_ROC_XGB_akima'\n",
    "title_plot= 'Confronto Curve ROC Medie - XGBoost'\n",
    "\n",
    "y_pred_proba_xgb = xgb_best_model.predict_proba(X_test)  # probabilità previste per il modello Random Forest con dati sbilanciati\n",
    "y_pred_proba_xgb_up = xgb_best_model_up.predict_proba(X_test_up)  # dati upsampled\n",
    "y_pred_proba_xgb_down = xgb_best_model_down.predict_proba(X_test_down)  # dati downsampled\n",
    "\n",
    "# plot la curva ROC media\n",
    "plot_average_roc_curve(\n",
    "    y_tests=[y_test, y_test_up, y_test_down],  # dati di test per ciascun modello\n",
    "    y_pred_probas=[y_pred_proba_xgb, y_pred_proba_xgb_up, y_pred_proba_xgb_down],  #probabilità previste per ciascun modello\n",
    "    titles=[\"Dati Sbilanciati\", \"Dati Upsampled\", \"Dati Downsampled\"], \n",
    "    class_labels=class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"progetto/curve_ROC_RF_akima.jpg\" alt=\"y\" style=\"width: 500px;\"/>\n",
    "<img src=\"progetto/curve_ROC_XGB_akima.jpg\" alt=\"y\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RETE NEURALE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per ricercare un algoritmo più performante, si preferisce adesse esplorare le reti neurali e come primo approccio si crea una rete tradizionale (ANN), quindi composta soltanto da layers densi. Nel caso in cui si analizza il dataset 'interpolated_curves' con i dati filtrati rispetto ai semestri di riposo, il numero di features è 546, pertanto impiego 4 layers densi andando riducendo progressivamente la dimensione di ciascuno, fino ad un output di 5, equivalente al numero di classi da classificare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#librerie reti neurali\n",
    "#si importano tutte prima nel caso l'utente volesse compilare una soltanto delle rete di seguito presentate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras import Sequential, regularizers, callbacks\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, BatchNormalization, GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mario Lauriano\\miniconda3\\envs\\machine_learning\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 - 3s - 8ms/step - accuracy: 0.5707 - loss: 1.0679 - val_accuracy: 0.7975 - val_loss: 0.7823\n",
      "Epoch 2/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.7963 - loss: 0.7293 - val_accuracy: 0.8163 - val_loss: 0.5664\n",
      "Epoch 3/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8212 - loss: 0.5288 - val_accuracy: 0.8425 - val_loss: 0.4780\n",
      "Epoch 4/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8532 - loss: 0.4266 - val_accuracy: 0.8381 - val_loss: 0.4692\n",
      "Epoch 5/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8679 - loss: 0.3698 - val_accuracy: 0.8619 - val_loss: 0.4013\n",
      "Epoch 6/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8867 - loss: 0.3236 - val_accuracy: 0.8606 - val_loss: 0.4098\n",
      "Epoch 7/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8976 - loss: 0.2906 - val_accuracy: 0.8656 - val_loss: 0.3891\n",
      "Epoch 8/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9112 - loss: 0.2609 - val_accuracy: 0.8687 - val_loss: 0.4014\n",
      "Epoch 9/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9173 - loss: 0.2404 - val_accuracy: 0.8644 - val_loss: 0.3979\n",
      "Epoch 10/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9254 - loss: 0.2192 - val_accuracy: 0.8637 - val_loss: 0.4142\n",
      "Epoch 11/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9347 - loss: 0.1991 - val_accuracy: 0.8650 - val_loss: 0.4082\n",
      "Epoch 12/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9410 - loss: 0.1821 - val_accuracy: 0.8706 - val_loss: 0.3983\n",
      "Epoch 13/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9450 - loss: 0.1766 - val_accuracy: 0.8731 - val_loss: 0.3984\n",
      "Epoch 14/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9544 - loss: 0.1521 - val_accuracy: 0.8756 - val_loss: 0.4020\n",
      "Epoch 15/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9588 - loss: 0.1385 - val_accuracy: 0.8831 - val_loss: 0.3978\n",
      "Epoch 16/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9616 - loss: 0.1288 - val_accuracy: 0.8744 - val_loss: 0.4389\n",
      "Epoch 17/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9627 - loss: 0.1240 - val_accuracy: 0.8725 - val_loss: 0.4242\n",
      "Epoch 18/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9688 - loss: 0.1112 - val_accuracy: 0.8781 - val_loss: 0.4235\n",
      "Epoch 19/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9735 - loss: 0.0969 - val_accuracy: 0.8781 - val_loss: 0.4256\n",
      "Epoch 20/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9771 - loss: 0.0925 - val_accuracy: 0.8794 - val_loss: 0.4344\n",
      "Epoch 21/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9770 - loss: 0.0862 - val_accuracy: 0.8737 - val_loss: 0.4366\n",
      "Epoch 22/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9803 - loss: 0.0782 - val_accuracy: 0.8719 - val_loss: 0.4760\n",
      "Epoch 23/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9828 - loss: 0.0713 - val_accuracy: 0.8750 - val_loss: 0.4756\n",
      "Epoch 24/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9835 - loss: 0.0671 - val_accuracy: 0.8813 - val_loss: 0.4662\n",
      "Epoch 25/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9843 - loss: 0.0644 - val_accuracy: 0.8813 - val_loss: 0.4492\n",
      "Epoch 26/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9854 - loss: 0.0602 - val_accuracy: 0.8756 - val_loss: 0.5075\n",
      "Epoch 27/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9866 - loss: 0.0559 - val_accuracy: 0.8831 - val_loss: 0.5013\n",
      "Epoch 28/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9861 - loss: 0.0614 - val_accuracy: 0.8744 - val_loss: 0.4846\n",
      "Epoch 29/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9892 - loss: 0.0513 - val_accuracy: 0.8763 - val_loss: 0.5300\n",
      "Epoch 30/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9939 - loss: 0.0381 - val_accuracy: 0.8781 - val_loss: 0.5412\n",
      "Epoch 31/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9895 - loss: 0.0439 - val_accuracy: 0.8788 - val_loss: 0.5264\n",
      "Epoch 32/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9935 - loss: 0.0346 - val_accuracy: 0.8794 - val_loss: 0.5380\n",
      "Epoch 33/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9913 - loss: 0.0374 - val_accuracy: 0.8662 - val_loss: 0.5894\n",
      "Epoch 34/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9922 - loss: 0.0359 - val_accuracy: 0.8769 - val_loss: 0.5554\n",
      "Epoch 35/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9927 - loss: 0.0328 - val_accuracy: 0.8737 - val_loss: 0.5747\n",
      "Epoch 36/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9928 - loss: 0.0320 - val_accuracy: 0.8719 - val_loss: 0.5959\n",
      "Epoch 37/50\n",
      "350/350 - 1s - 2ms/step - accuracy: 0.9944 - loss: 0.0270 - val_accuracy: 0.8712 - val_loss: 0.5853\n",
      "Epoch 38/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9905 - loss: 0.0347 - val_accuracy: 0.8763 - val_loss: 0.6034\n",
      "Epoch 39/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9939 - loss: 0.0293 - val_accuracy: 0.8769 - val_loss: 0.6034\n",
      "Epoch 40/50\n",
      "350/350 - 1s - 2ms/step - accuracy: 0.9955 - loss: 0.0235 - val_accuracy: 0.8725 - val_loss: 0.6293\n",
      "Epoch 41/50\n",
      "350/350 - 1s - 2ms/step - accuracy: 0.9947 - loss: 0.0251 - val_accuracy: 0.8763 - val_loss: 0.6317\n",
      "Epoch 42/50\n",
      "350/350 - 1s - 2ms/step - accuracy: 0.9944 - loss: 0.0257 - val_accuracy: 0.8781 - val_loss: 0.6672\n",
      "Epoch 43/50\n",
      "350/350 - 1s - 2ms/step - accuracy: 0.9922 - loss: 0.0287 - val_accuracy: 0.8775 - val_loss: 0.6539\n",
      "Epoch 44/50\n",
      "350/350 - 1s - 2ms/step - accuracy: 0.9963 - loss: 0.0183 - val_accuracy: 0.8769 - val_loss: 0.7083\n",
      "Epoch 45/50\n",
      "350/350 - 1s - 2ms/step - accuracy: 0.9979 - loss: 0.0143 - val_accuracy: 0.8737 - val_loss: 0.6565\n",
      "Epoch 46/50\n",
      "350/350 - 1s - 2ms/step - accuracy: 0.9970 - loss: 0.0162 - val_accuracy: 0.8719 - val_loss: 0.6938\n",
      "Epoch 47/50\n",
      "350/350 - 1s - 2ms/step - accuracy: 0.9920 - loss: 0.0310 - val_accuracy: 0.8769 - val_loss: 0.7129\n",
      "Epoch 48/50\n",
      "350/350 - 1s - 2ms/step - accuracy: 0.9967 - loss: 0.0176 - val_accuracy: 0.8781 - val_loss: 0.7060\n",
      "Epoch 49/50\n",
      "350/350 - 1s - 2ms/step - accuracy: 0.9976 - loss: 0.0135 - val_accuracy: 0.8694 - val_loss: 0.7452\n",
      "Epoch 50/50\n",
      "350/350 - 1s - 2ms/step - accuracy: 0.9968 - loss: 0.0154 - val_accuracy: 0.8750 - val_loss: 0.7219\n",
      "63/63 - 0s - 3ms/step - accuracy: 0.8740 - loss: 0.6701\n",
      "\n",
      "Test Accuracy: 0.8740\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "#sempre per evitare il data-leakage si crea un nuovo df per la ANN\n",
    "classes = [entry.split(',')[3].split(':')[0].strip() for entry in data['sim_type']]\n",
    "df_ANN_up = pd.DataFrame(interpolated_curves, columns=[f'feature_{i}' for i in range(np.array(interpolated_curves).shape[1])])\n",
    "df_ANN_up['target'] = classes\n",
    "\n",
    "#estrazione delle feature e del target\n",
    "X_up = df_ANN_up.drop(columns=['target']).values\n",
    "y_up = df_ANN_up['target'].values\n",
    "\n",
    "#standardizzazione feature\n",
    "scaler = StandardScaler()\n",
    "X_up_scal = scaler.fit_transform(X_up)\n",
    "\n",
    "#encoding delle etichette\n",
    "y_up_enc = pd.get_dummies(y_up, dtype=int).values\n",
    "\n",
    "# divisione dei dati in training e test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_up_scal, y_up_enc, test_size=0.2, random_state=42)\n",
    "\n",
    "# divisione del training set in un set di addestramento e un set di validazione\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "#data augmentation sul set di addestramento\n",
    "smote = SMOTE(sampling_strategy='not majority', random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# costruzione di una rete neurale tradizionale senza tecniche anti-overfitting\n",
    "model_ann1 = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train_res.shape[1],)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(5, activation='softmax')  # Output con 5 classi\n",
    "])\n",
    "\n",
    "# compilazione del modello\n",
    "model_ann1.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# addestramento del modello\n",
    "# utilizza il set di validazione per monitorare l'accuratezza del modello durante l'addestramento\n",
    "history_ann1 = model_ann1.fit(\n",
    "    x=X_train_res, y=y_train_res,\n",
    "    validation_data=(X_val, y_val),  # set di validazione esplicito\n",
    "    epochs=50,\n",
    "    verbose=2)\n",
    "\n",
    "# valutazione sul test set\n",
    "test_loss, test_acc = model_ann1.evaluate(X_test, y_test, verbose=2)\n",
    "print(f'\\nTest Accuracy: {test_acc:.4f}')\n",
    "\n",
    "#previsioni sul test set\n",
    "y_pred_ann1 = np.argmax(model_ann1.predict(X_test), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Funzione per valutare le prestazioni di una rete neurale ###\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "def evaluate_model_performance(model, X_test, y_test, y_pred, history, model_name='neural_network'):\n",
    "    \"\"\"\n",
    "    Genera la matrice di confusione, lo schema della rete e il grafico della loss durante l'addestramento.\n",
    "\n",
    "    Parametri\n",
    "        model (tf.keras.Model): modello di rete neurale addestrato;\n",
    "        X_test (np.array): dati di test;\n",
    "        y_test (np.array): etichette di test in one-hot encoding;\n",
    "        y_pred (np.array): etichette predette nel formato scalare;\n",
    "        history (tf.keras.callbacks.History): oggetto della cronologia di addestramento;\n",
    "        model_name (str): Nome per salvare i file di output (default: 'neural_network').\n",
    "    \"\"\"\n",
    "    # conversione delle etichette di test in formato scalare, necessario per la matrice di confusione\n",
    "    y_test_scalar = np.argmax(y_test, axis=1)\n",
    "\n",
    "    # generazione della matrice di confusione\n",
    "    conf_mat = confusion_matrix(y_test_scalar, y_pred)\n",
    "    print(f'Matrice di Confusione {model_name}:')\n",
    "    print(conf_mat)\n",
    "\n",
    "    ## plot dello schema della rete neurale (la figura inserita prima) ##\n",
    "    plot_model(model, show_shapes=True, to_file=f'progetto\\\\{model_name}_structure.png')\n",
    "    \n",
    "    # plot dell'andamento della curva loss durante l'addestramento\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(history.history['loss'], label='Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoche')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Curva di apprendimento - {model_name}')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'progetto\\\\{model_name}_training_loss.png', dpi=72)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice di Confusione rete_tradizionale_masked:\n",
      "[[251   0  20   0   0]\n",
      " [  1 710   0   0   5]\n",
      " [ 32   1 547  10   2]\n",
      " [  2  13  28 168  63]\n",
      " [  2   6   7  60  72]]\n"
     ]
    }
   ],
   "source": [
    "# valutazione del modello e generazione dei grafici e della matrice di confusione\n",
    "evaluate_model_performance(model=model_ann1, X_test=X_test, y_test=y_test, y_pred=y_pred_ann1, history=history_ann1, model_name='rete_tradizionale_masked')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dalla curva di apprendimento si osserva come nonostante la rete impari bene dai dati di training non riesca poi a generalizzare ai dati del set di validation, perché overfitta sui dati di training. Vedremo adesso come risolvere tale problema.\n",
    "\n",
    "<img src=\"progetto/rete tradizionale_masked_training_loss.png\" alt=\"y\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prima di proseguire, si ripropone la stessa rete con il dataset interpolato, ma non filtrato sui semestri di riposo, in prospettiva di utilizzare una RNN. Tale test vorrà appurare come i valori flaggati dei semestri di riposo non disturbino (se non per tempo computazionale) la rete, mostrando dati confrontabili con la prima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## richiamo la funzione e stampo le curve ROC per XGBoost ##\n",
    "\n",
    "filename='curve_ROC_XGB_akima'\n",
    "title_plot= 'Confronto Curve ROC Medie - XGBoost'\n",
    "\n",
    "y_pred_proba_ann1 = xgb_best_model.predict_proba(X_test)\n",
    "\n",
    "# plot la curva ROC media\n",
    "plot_average_roc_curve(\n",
    "    y_tests=[y_test, y_test_up, y_test_down],  # dati di test per ciascun modello\n",
    "    y_pred_probas=[y_pred_proba_xgb, y_pred_proba_xgb_up, y_pred_proba_xgb_down],  #probabilità previste per ciascun modello\n",
    "    titles=[\"Dati Sbilanciati\", \"Dati Upsampled\", \"Dati Downsampled\"], \n",
    "    class_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mario Lauriano\\miniconda3\\envs\\machine_learning\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 - 2s - 7ms/step - accuracy: 0.5985 - loss: 1.0421 - val_accuracy: 0.8144 - val_loss: 0.7323\n",
      "Epoch 2/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8016 - loss: 0.6978 - val_accuracy: 0.8313 - val_loss: 0.5270\n",
      "Epoch 3/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8241 - loss: 0.5047 - val_accuracy: 0.8494 - val_loss: 0.4482\n",
      "Epoch 4/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8487 - loss: 0.4175 - val_accuracy: 0.8650 - val_loss: 0.3924\n",
      "Epoch 5/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8724 - loss: 0.3614 - val_accuracy: 0.8619 - val_loss: 0.3868\n",
      "Epoch 6/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8857 - loss: 0.3208 - val_accuracy: 0.8544 - val_loss: 0.3829\n",
      "Epoch 7/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8972 - loss: 0.2898 - val_accuracy: 0.8750 - val_loss: 0.3453\n",
      "Epoch 8/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9055 - loss: 0.2717 - val_accuracy: 0.8694 - val_loss: 0.3487\n",
      "Epoch 9/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9137 - loss: 0.2421 - val_accuracy: 0.8706 - val_loss: 0.3556\n",
      "Epoch 10/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9238 - loss: 0.2198 - val_accuracy: 0.8650 - val_loss: 0.3713\n",
      "Epoch 11/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9304 - loss: 0.2055 - val_accuracy: 0.8675 - val_loss: 0.3719\n",
      "Epoch 12/50\n",
      "350/350 - 1s - 4ms/step - accuracy: 0.9404 - loss: 0.1867 - val_accuracy: 0.8644 - val_loss: 0.3651\n",
      "Epoch 13/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9438 - loss: 0.1714 - val_accuracy: 0.8700 - val_loss: 0.3521\n",
      "Epoch 14/50\n",
      "350/350 - 1s - 4ms/step - accuracy: 0.9522 - loss: 0.1571 - val_accuracy: 0.8694 - val_loss: 0.3526\n",
      "Epoch 15/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9566 - loss: 0.1442 - val_accuracy: 0.8750 - val_loss: 0.3659\n",
      "Epoch 16/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9619 - loss: 0.1309 - val_accuracy: 0.8681 - val_loss: 0.3762\n",
      "Epoch 17/50\n",
      "350/350 - 1s - 4ms/step - accuracy: 0.9646 - loss: 0.1206 - val_accuracy: 0.8744 - val_loss: 0.3706\n",
      "Epoch 18/50\n",
      "350/350 - 1s - 4ms/step - accuracy: 0.9686 - loss: 0.1160 - val_accuracy: 0.8681 - val_loss: 0.3730\n",
      "Epoch 19/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9726 - loss: 0.1015 - val_accuracy: 0.8637 - val_loss: 0.3827\n",
      "Epoch 20/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9750 - loss: 0.0936 - val_accuracy: 0.8769 - val_loss: 0.3840\n",
      "Epoch 21/50\n",
      "350/350 - 1s - 4ms/step - accuracy: 0.9724 - loss: 0.0946 - val_accuracy: 0.8731 - val_loss: 0.3864\n",
      "Epoch 22/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9789 - loss: 0.0810 - val_accuracy: 0.8637 - val_loss: 0.4156\n",
      "Epoch 23/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9818 - loss: 0.0742 - val_accuracy: 0.8600 - val_loss: 0.4360\n",
      "Epoch 24/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9833 - loss: 0.0703 - val_accuracy: 0.8625 - val_loss: 0.4240\n",
      "Epoch 25/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9839 - loss: 0.0662 - val_accuracy: 0.8469 - val_loss: 0.4775\n",
      "Epoch 26/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9853 - loss: 0.0592 - val_accuracy: 0.8431 - val_loss: 0.5089\n",
      "Epoch 27/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9862 - loss: 0.0583 - val_accuracy: 0.8737 - val_loss: 0.4454\n",
      "Epoch 28/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9864 - loss: 0.0609 - val_accuracy: 0.8806 - val_loss: 0.4491\n",
      "Epoch 29/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9889 - loss: 0.0540 - val_accuracy: 0.8687 - val_loss: 0.4870\n",
      "Epoch 30/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9908 - loss: 0.0455 - val_accuracy: 0.8750 - val_loss: 0.4499\n",
      "Epoch 31/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9914 - loss: 0.0403 - val_accuracy: 0.8775 - val_loss: 0.4502\n",
      "Epoch 32/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9863 - loss: 0.0536 - val_accuracy: 0.8600 - val_loss: 0.5068\n",
      "Epoch 33/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9917 - loss: 0.0374 - val_accuracy: 0.8769 - val_loss: 0.4768\n",
      "Epoch 34/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9918 - loss: 0.0365 - val_accuracy: 0.8569 - val_loss: 0.4960\n",
      "Epoch 35/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9943 - loss: 0.0317 - val_accuracy: 0.8769 - val_loss: 0.5041\n",
      "Epoch 36/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9954 - loss: 0.0276 - val_accuracy: 0.8731 - val_loss: 0.4982\n",
      "Epoch 37/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9923 - loss: 0.0307 - val_accuracy: 0.8737 - val_loss: 0.5119\n",
      "Epoch 38/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9920 - loss: 0.0341 - val_accuracy: 0.8731 - val_loss: 0.5661\n",
      "Epoch 39/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9911 - loss: 0.0336 - val_accuracy: 0.8694 - val_loss: 0.5500\n",
      "Epoch 40/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9965 - loss: 0.0219 - val_accuracy: 0.8750 - val_loss: 0.5242\n",
      "Epoch 41/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9928 - loss: 0.0274 - val_accuracy: 0.8694 - val_loss: 0.5898\n",
      "Epoch 42/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9950 - loss: 0.0228 - val_accuracy: 0.8700 - val_loss: 0.5886\n",
      "Epoch 43/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9936 - loss: 0.0266 - val_accuracy: 0.8606 - val_loss: 0.5839\n",
      "Epoch 44/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9895 - loss: 0.0425 - val_accuracy: 0.8456 - val_loss: 0.6342\n",
      "Epoch 45/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9970 - loss: 0.0185 - val_accuracy: 0.8725 - val_loss: 0.5696\n",
      "Epoch 46/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9970 - loss: 0.0183 - val_accuracy: 0.8750 - val_loss: 0.5643\n",
      "Epoch 47/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9962 - loss: 0.0188 - val_accuracy: 0.8781 - val_loss: 0.5682\n",
      "Epoch 48/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9933 - loss: 0.0268 - val_accuracy: 0.8781 - val_loss: 0.5807\n",
      "Epoch 49/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9970 - loss: 0.0155 - val_accuracy: 0.8775 - val_loss: 0.5894\n",
      "Epoch 50/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9925 - loss: 0.0254 - val_accuracy: 0.8744 - val_loss: 0.6016\n",
      "63/63 - 0s - 3ms/step - accuracy: 0.8735 - loss: 0.6360\n",
      "\n",
      "Test Accuracy: 0.8735\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "## rete neurale tradizionale senza maschera stagionale, ma utilizzando i valori -99 nei buchi dovuti ai semestri di riposo ##\n",
    "full_df_ANN = pd.read_csv('interp_lc_5sem.csv')\n",
    "X = full_df_ANN.drop(columns=['target']).values\n",
    "y = full_df_ANN['target'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scal = scaler.fit_transform(X)\n",
    "\n",
    "y_enc = pd.get_dummies(y, dtype=int).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scal, y_enc, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "smote = SMOTE(sampling_strategy='not majority', random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "model_ann2 = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train_res.shape[1],)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "model_ann2.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_ann2 = model_ann2.fit(\n",
    "    x=X_train_res, y=y_train_res,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    verbose=2)\n",
    "\n",
    "test_loss, test_acc = model_ann2.evaluate(X_test, y_test, verbose=2)\n",
    "print(f'\\nTest Accuracy: {test_acc:.4f}')\n",
    "y_pred_ann2 = np.argmax(model_ann2.predict(X_test), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice di Confusione rete tradizionale full_lc:\n",
      "[[242   0  28   0   1]\n",
      " [  1 707   0   5   3]\n",
      " [ 30   1 553   7   1]\n",
      " [  2  12  31 175  54]\n",
      " [  4   4   3  66  70]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_performance(model=model_ann2, X_test=X_test, y_test=y_test, y_pred=y_pred_ann2, history=history_ann2, model_name='rete tradizionale full_lc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matrice di confusione con questo dataset è pienamente confrontabile con la prima; inoltre, l'accuratezza media è addirittura migliore. Pertanto si sceglie di proseguire con tale dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting \n",
    "\n",
    "L'overfitting è causato dal fatto che un modello tende a \"memorizzare\" specifiche caratteristiche dei dati di training, compresi rumore e anomalie. Questo comportamento porta a un aumento delle prestazioni apparenti sui dati di validazione, soprattutto se il set di validazione è simile al set di addestramento (cosa plausibile in questo caso dato il data augmentation), ma il modello sarà meno capace di generalizzare.\n",
    "\n",
    "È possibile adottare delle strategie per contrastarlo, ad esempio funzioni di regolazione:\n",
    "\n",
    "*Regolarizzazione L2*: penalizza i pesi troppo grandi nel modello aggiungendo una \"penalità\" alla funzione di perdita. Questa penalità è proporzionale al quadrato dei valori dei pesi (da cui il nome L2). In pratica, L2 riduce la complessità del modello;\n",
    "\n",
    "*Dropout*: disattiva casualmente una percentuale di neuroni in ogni strato durante l'allenamento, rendendo il modello meno dipendente da particolari neuroni. Questo \"spegnimento\" aiuta a evitare che il modello memorizzi i dati di allenamento (overfitting), poiché ogni iterazione costringe la rete a utilizzare percorsi diversi per apprendere;\n",
    "\n",
    "*ReduceLROnPlateau*: riduce il tasso di apprendimento se la *val_loss* smette di migliorare per un certo numero di epoche consecutive. Infatti, un learning rate troppo alto può causare oscillazioni o impedire il raggiungimento del minimo locale della funzione di perdita, portando a overfitting o a un apprendimento instabile. Riducendo progressivamente il learning rate, il modello può adattarsi in modo più fine alla superficie della funzione di perdita, migliorando la generalizzazione;\n",
    "\n",
    "*ModelCheckpoint*: salva il modello solo quando ottiene il miglior risultato sulla *val_loss*;\n",
    "\n",
    "*Early Stopping*: l’allenamento si interrompe se la *val_loss* non migliora per 10 epoche consecutive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mario Lauriano\\miniconda3\\envs\\machine_learning\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 - 3s - 8ms/step - accuracy: 0.4305 - loss: 1.8499 - val_accuracy: 0.6994 - val_loss: 1.4082 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "350/350 - 1s - 4ms/step - accuracy: 0.6289 - loss: 1.5333 - val_accuracy: 0.8175 - val_loss: 1.1934 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "350/350 - 1s - 4ms/step - accuracy: 0.7124 - loss: 1.3485 - val_accuracy: 0.8256 - val_loss: 1.0477 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "350/350 - 1s - 4ms/step - accuracy: 0.7381 - loss: 1.2042 - val_accuracy: 0.8313 - val_loss: 0.9505 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "350/350 - 1s - 4ms/step - accuracy: 0.7481 - loss: 1.1197 - val_accuracy: 0.8344 - val_loss: 0.9049 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "350/350 - 1s - 4ms/step - accuracy: 0.7614 - loss: 1.0480 - val_accuracy: 0.8313 - val_loss: 0.8717 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "350/350 - 1s - 4ms/step - accuracy: 0.7743 - loss: 0.9995 - val_accuracy: 0.8450 - val_loss: 0.8547 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "350/350 - 1s - 4ms/step - accuracy: 0.7800 - loss: 0.9680 - val_accuracy: 0.8388 - val_loss: 0.8299 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "350/350 - 1s - 4ms/step - accuracy: 0.7945 - loss: 0.9385 - val_accuracy: 0.8581 - val_loss: 0.8007 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "350/350 - 1s - 4ms/step - accuracy: 0.7934 - loss: 0.9117 - val_accuracy: 0.8537 - val_loss: 0.8010 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "350/350 - 1s - 4ms/step - accuracy: 0.8022 - loss: 0.8878 - val_accuracy: 0.8612 - val_loss: 0.7744 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8046 - loss: 0.8651 - val_accuracy: 0.8556 - val_loss: 0.7703 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "350/350 - 1s - 4ms/step - accuracy: 0.8104 - loss: 0.8461 - val_accuracy: 0.8606 - val_loss: 0.7613 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "350/350 - 1s - 4ms/step - accuracy: 0.8201 - loss: 0.8207 - val_accuracy: 0.8537 - val_loss: 0.7459 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "350/350 - 1s - 4ms/step - accuracy: 0.8237 - loss: 0.8029 - val_accuracy: 0.8594 - val_loss: 0.7363 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "350/350 - 1s - 4ms/step - accuracy: 0.8237 - loss: 0.7916 - val_accuracy: 0.8581 - val_loss: 0.7305 - learning_rate: 1.0000e-04\n",
      "Epoch 17/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8251 - loss: 0.7810 - val_accuracy: 0.8556 - val_loss: 0.7195 - learning_rate: 1.0000e-04\n",
      "Epoch 18/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8321 - loss: 0.7541 - val_accuracy: 0.8612 - val_loss: 0.7132 - learning_rate: 1.0000e-04\n",
      "Epoch 19/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8393 - loss: 0.7417 - val_accuracy: 0.8562 - val_loss: 0.7128 - learning_rate: 1.0000e-04\n",
      "Epoch 20/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8428 - loss: 0.7225 - val_accuracy: 0.8600 - val_loss: 0.7053 - learning_rate: 1.0000e-04\n",
      "Epoch 21/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8444 - loss: 0.7081 - val_accuracy: 0.8531 - val_loss: 0.6977 - learning_rate: 1.0000e-04\n",
      "Epoch 22/50\n",
      "350/350 - 1s - 4ms/step - accuracy: 0.8497 - loss: 0.6942 - val_accuracy: 0.8581 - val_loss: 0.6914 - learning_rate: 1.0000e-04\n",
      "Epoch 23/50\n",
      "350/350 - 1s - 4ms/step - accuracy: 0.8543 - loss: 0.6789 - val_accuracy: 0.8594 - val_loss: 0.6892 - learning_rate: 1.0000e-04\n",
      "Epoch 24/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8546 - loss: 0.6652 - val_accuracy: 0.8594 - val_loss: 0.6868 - learning_rate: 1.0000e-04\n",
      "Epoch 25/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8605 - loss: 0.6493 - val_accuracy: 0.8694 - val_loss: 0.6755 - learning_rate: 1.0000e-04\n",
      "Epoch 26/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8679 - loss: 0.6364 - val_accuracy: 0.8669 - val_loss: 0.6681 - learning_rate: 1.0000e-04\n",
      "Epoch 27/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8657 - loss: 0.6292 - val_accuracy: 0.8587 - val_loss: 0.6651 - learning_rate: 1.0000e-04\n",
      "Epoch 28/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8676 - loss: 0.6153 - val_accuracy: 0.8612 - val_loss: 0.6567 - learning_rate: 1.0000e-04\n",
      "Epoch 29/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8698 - loss: 0.6051 - val_accuracy: 0.8637 - val_loss: 0.6771 - learning_rate: 1.0000e-04\n",
      "Epoch 30/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8761 - loss: 0.5924 - val_accuracy: 0.8637 - val_loss: 0.6586 - learning_rate: 1.0000e-04\n",
      "Epoch 31/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8756 - loss: 0.5855 - val_accuracy: 0.8631 - val_loss: 0.6460 - learning_rate: 1.0000e-04\n",
      "Epoch 32/50\n",
      "350/350 - 1s - 4ms/step - accuracy: 0.8777 - loss: 0.5737 - val_accuracy: 0.8681 - val_loss: 0.6421 - learning_rate: 1.0000e-04\n",
      "Epoch 33/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8775 - loss: 0.5652 - val_accuracy: 0.8550 - val_loss: 0.6420 - learning_rate: 1.0000e-04\n",
      "Epoch 34/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8824 - loss: 0.5597 - val_accuracy: 0.8562 - val_loss: 0.6629 - learning_rate: 1.0000e-04\n",
      "Epoch 35/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8847 - loss: 0.5455 - val_accuracy: 0.8612 - val_loss: 0.6351 - learning_rate: 1.0000e-04\n",
      "Epoch 36/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8889 - loss: 0.5376 - val_accuracy: 0.8587 - val_loss: 0.6515 - learning_rate: 1.0000e-04\n",
      "Epoch 37/50\n",
      "350/350 - 1s - 4ms/step - accuracy: 0.8914 - loss: 0.5267 - val_accuracy: 0.8600 - val_loss: 0.6412 - learning_rate: 1.0000e-04\n",
      "Epoch 38/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8853 - loss: 0.5291 - val_accuracy: 0.8562 - val_loss: 0.6476 - learning_rate: 1.0000e-04\n",
      "Epoch 39/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8946 - loss: 0.5206 - val_accuracy: 0.8569 - val_loss: 0.6537 - learning_rate: 1.0000e-04\n",
      "Epoch 40/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.8930 - loss: 0.5095 - val_accuracy: 0.8625 - val_loss: 0.6443 - learning_rate: 1.0000e-04\n",
      "Epoch 41/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9058 - loss: 0.4878 - val_accuracy: 0.8631 - val_loss: 0.6343 - learning_rate: 5.0000e-05\n",
      "Epoch 42/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9033 - loss: 0.4834 - val_accuracy: 0.8650 - val_loss: 0.6304 - learning_rate: 5.0000e-05\n",
      "Epoch 43/50\n",
      "350/350 - 1s - 4ms/step - accuracy: 0.9057 - loss: 0.4780 - val_accuracy: 0.8600 - val_loss: 0.6267 - learning_rate: 5.0000e-05\n",
      "Epoch 44/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9063 - loss: 0.4711 - val_accuracy: 0.8662 - val_loss: 0.6322 - learning_rate: 5.0000e-05\n",
      "Epoch 45/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9059 - loss: 0.4708 - val_accuracy: 0.8631 - val_loss: 0.6351 - learning_rate: 5.0000e-05\n",
      "Epoch 46/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9086 - loss: 0.4652 - val_accuracy: 0.8619 - val_loss: 0.6329 - learning_rate: 5.0000e-05\n",
      "Epoch 47/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9118 - loss: 0.4607 - val_accuracy: 0.8656 - val_loss: 0.6258 - learning_rate: 5.0000e-05\n",
      "Epoch 48/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9126 - loss: 0.4550 - val_accuracy: 0.8656 - val_loss: 0.6214 - learning_rate: 5.0000e-05\n",
      "Epoch 49/50\n",
      "350/350 - 1s - 3ms/step - accuracy: 0.9080 - loss: 0.4571 - val_accuracy: 0.8631 - val_loss: 0.6140 - learning_rate: 5.0000e-05\n",
      "Epoch 50/50\n",
      "350/350 - 1s - 4ms/step - accuracy: 0.9147 - loss: 0.4494 - val_accuracy: 0.8700 - val_loss: 0.6055 - learning_rate: 5.0000e-05\n",
      "63/63 - 0s - 4ms/step - accuracy: 0.8740 - loss: 0.5808\n",
      "\n",
      "Test Accuracy: 0.8740\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "## RETE ANTI-OVERFITTING ##\n",
    "\n",
    "full_df_ANN = pd.read_csv('interp_lc_5sem.csv')\n",
    "X_up = full_df_ANN.drop(columns=['target']).values\n",
    "y_up = full_df_ANN['target'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_up_scal = scaler.fit_transform(X_up)\n",
    "\n",
    "y_up_enc = pd.get_dummies(y_up, dtype=int).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_up_scal, y_up_enc, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "smote = SMOTE(sampling_strategy='not majority', random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "model_ann3 = Sequential([\n",
    "    Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001), input_shape=(X_train_res.shape[1],)),\n",
    "    Dropout(0.2),  # Dropout del 50% dei neuroni\n",
    "    \n",
    "    Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "model_ann3.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "checkpoint = ModelCheckpoint('best_model_cnn_lstm.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "history_ann3 = model_ann3.fit(\n",
    "    x=X_train_res, y=y_train_res,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, lr_scheduler, checkpoint],\n",
    "    verbose=2)\n",
    "\n",
    "test_loss, test_acc = model_ann3.evaluate(X_test, y_test, verbose=2)\n",
    "print(f'\\nTest Accuracy: {test_acc:.4f}')\n",
    "y_pred_ann3 = np.argmax(model_ann3.predict(X_test), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice di Confusione rete tradizionale_regul_overfitting:\n",
      "[[249   0  20   0   2]\n",
      " [  0 712   0   2   2]\n",
      " [ 40   1 545   6   0]\n",
      " [  2  12  26 152  82]\n",
      " [  3   8   2  44  90]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_performance(model=model_ann3, X_test=X_test, y_test=y_test, y_pred=y_pred_ann3, history=history_ann3, model_name='rete tradizionale_regul_overfitting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con le regolazioni aggiunte si osserva una migliore capacità di generalizzazione ai dati di validazione. \n",
    "\n",
    "<img src=\"progetto/rete tradizionale_regul_overfitting_training_loss.png\" alt=\"y\" style=\"width: 500px;\"/>\n",
    "\n",
    "È comunque evidente che ancora c'è da migliorare per rendere la rete capace di generalizzare meglio: un modo per farlo è semplificare il modello riducendo il numero di strati e/o diminuendo il numero di neuroni di ciascuno.\n",
    "\n",
    "Inoltre, poiché le curve di luce sono ordinate temporalmente, si preferisce di impiegare layers convoluzionali, utili a trovare pattern nel breve periodo all'interno dei dati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rete Neurale Convoluzionale (CNN)\n",
    "\n",
    "Le reti neurali convoluzionali si distinguono da quelle tradizionali per le loro prestazioni superiori con immagini, input vocali e segnali unidimensionali (quale è una curva di luce). Sono principalmente suddivise in tre tipi di livelli, ovvero:\n",
    "\n",
    "- convoluzionali, dove è presente un rilevatore di funzioni, definito anche kernel o filtro, che 'scorrerà' sull'array, verificando la presenza di pattern nel breve periodo (questo processo è noto come convoluzione);\n",
    "- di pooling, una sorta di filtro che seleziona il punto con il valore massimo da inviare all'array di output;\n",
    "- completamente connesso (FC, Fully-connected), ne serve almneo uno perché permette di connettere l'output direttamente a un nodo nel livello precedente.\n",
    "\n",
    "*Attenzione!* I  primi layers convoluzionali hanno meno neuroni di quelli successivi, come a formare una 'gerarchia': i primi osserveranno le caratteristiche più superficiali del modello e quelli successivi andranno più nel dettaglio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "350/350 - 8s - 23ms/step - accuracy: 0.5435 - loss: 1.2990 - val_accuracy: 0.7506 - val_loss: 0.8514 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "350/350 - 6s - 18ms/step - accuracy: 0.6414 - loss: 0.9856 - val_accuracy: 0.8169 - val_loss: 0.7754 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "350/350 - 6s - 18ms/step - accuracy: 0.6572 - loss: 0.9428 - val_accuracy: 0.7875 - val_loss: 0.7320 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "350/350 - 7s - 20ms/step - accuracy: 0.6680 - loss: 0.9134 - val_accuracy: 0.8119 - val_loss: 0.6838 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "350/350 - 7s - 19ms/step - accuracy: 0.6777 - loss: 0.8901 - val_accuracy: 0.8163 - val_loss: 0.6402 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "350/350 - 7s - 20ms/step - accuracy: 0.6819 - loss: 0.8740 - val_accuracy: 0.8269 - val_loss: 0.6351 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "350/350 - 7s - 20ms/step - accuracy: 0.6826 - loss: 0.8676 - val_accuracy: 0.8225 - val_loss: 0.6419 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "350/350 - 7s - 19ms/step - accuracy: 0.6889 - loss: 0.8493 - val_accuracy: 0.8006 - val_loss: 0.6692 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "350/350 - 7s - 19ms/step - accuracy: 0.6901 - loss: 0.8464 - val_accuracy: 0.8438 - val_loss: 0.6133 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "350/350 - 7s - 21ms/step - accuracy: 0.6931 - loss: 0.8299 - val_accuracy: 0.8413 - val_loss: 0.6039 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "350/350 - 8s - 22ms/step - accuracy: 0.6942 - loss: 0.8264 - val_accuracy: 0.8163 - val_loss: 0.6216 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "350/350 - 7s - 20ms/step - accuracy: 0.7009 - loss: 0.8197 - val_accuracy: 0.8062 - val_loss: 0.6420 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "350/350 - 6s - 18ms/step - accuracy: 0.7050 - loss: 0.8116 - val_accuracy: 0.8294 - val_loss: 0.6121 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "350/350 - 7s - 20ms/step - accuracy: 0.7048 - loss: 0.8131 - val_accuracy: 0.8425 - val_loss: 0.5784 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "350/350 - 7s - 21ms/step - accuracy: 0.7087 - loss: 0.7971 - val_accuracy: 0.8294 - val_loss: 0.6089 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "350/350 - 7s - 21ms/step - accuracy: 0.7019 - loss: 0.7994 - val_accuracy: 0.8306 - val_loss: 0.5943 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "350/350 - 7s - 20ms/step - accuracy: 0.7047 - loss: 0.7938 - val_accuracy: 0.8075 - val_loss: 0.6783 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "350/350 - 7s - 20ms/step - accuracy: 0.7076 - loss: 0.7898 - val_accuracy: 0.8381 - val_loss: 0.5467 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "350/350 - 7s - 19ms/step - accuracy: 0.7056 - loss: 0.7927 - val_accuracy: 0.8150 - val_loss: 0.6130 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "350/350 - 7s - 20ms/step - accuracy: 0.7100 - loss: 0.7905 - val_accuracy: 0.8350 - val_loss: 0.5633 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "350/350 - 7s - 19ms/step - accuracy: 0.7137 - loss: 0.7809 - val_accuracy: 0.8462 - val_loss: 0.5571 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "350/350 - 7s - 19ms/step - accuracy: 0.7076 - loss: 0.7883 - val_accuracy: 0.8388 - val_loss: 0.5558 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "350/350 - 7s - 20ms/step - accuracy: 0.7117 - loss: 0.7813 - val_accuracy: 0.8275 - val_loss: 0.6131 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "350/350 - 7s - 19ms/step - accuracy: 0.7241 - loss: 0.7457 - val_accuracy: 0.8481 - val_loss: 0.5226 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "350/350 - 7s - 19ms/step - accuracy: 0.7248 - loss: 0.7407 - val_accuracy: 0.8381 - val_loss: 0.5123 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "350/350 - 7s - 19ms/step - accuracy: 0.7301 - loss: 0.7330 - val_accuracy: 0.8556 - val_loss: 0.5101 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "350/350 - 6s - 18ms/step - accuracy: 0.7351 - loss: 0.7167 - val_accuracy: 0.8338 - val_loss: 0.5251 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "350/350 - 7s - 20ms/step - accuracy: 0.7334 - loss: 0.7122 - val_accuracy: 0.8225 - val_loss: 0.5082 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "350/350 - 7s - 19ms/step - accuracy: 0.7309 - loss: 0.7100 - val_accuracy: 0.8394 - val_loss: 0.4921 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "350/350 - 7s - 19ms/step - accuracy: 0.7296 - loss: 0.7110 - val_accuracy: 0.8487 - val_loss: 0.4945 - learning_rate: 5.0000e-04\n",
      "Epoch 31/50\n",
      "350/350 - 6s - 19ms/step - accuracy: 0.7403 - loss: 0.7079 - val_accuracy: 0.8256 - val_loss: 0.5322 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "350/350 - 7s - 19ms/step - accuracy: 0.7379 - loss: 0.7019 - val_accuracy: 0.8481 - val_loss: 0.5034 - learning_rate: 5.0000e-04\n",
      "Epoch 33/50\n",
      "350/350 - 7s - 19ms/step - accuracy: 0.7408 - loss: 0.6977 - val_accuracy: 0.8444 - val_loss: 0.4909 - learning_rate: 5.0000e-04\n",
      "Epoch 34/50\n",
      "350/350 - 6s - 18ms/step - accuracy: 0.7394 - loss: 0.6988 - val_accuracy: 0.8194 - val_loss: 0.5230 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "350/350 - 6s - 18ms/step - accuracy: 0.7376 - loss: 0.6946 - val_accuracy: 0.8425 - val_loss: 0.5002 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "350/350 - 7s - 19ms/step - accuracy: 0.7380 - loss: 0.6931 - val_accuracy: 0.8400 - val_loss: 0.4836 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "350/350 - 7s - 19ms/step - accuracy: 0.7395 - loss: 0.6964 - val_accuracy: 0.8350 - val_loss: 0.4947 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "350/350 - 7s - 19ms/step - accuracy: 0.7371 - loss: 0.6952 - val_accuracy: 0.8331 - val_loss: 0.4975 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "350/350 - 7s - 19ms/step - accuracy: 0.7379 - loss: 0.6983 - val_accuracy: 0.8225 - val_loss: 0.5279 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "350/350 - 7s - 19ms/step - accuracy: 0.7432 - loss: 0.6848 - val_accuracy: 0.8375 - val_loss: 0.4964 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "350/350 - 7s - 19ms/step - accuracy: 0.7411 - loss: 0.6852 - val_accuracy: 0.8344 - val_loss: 0.5202 - learning_rate: 5.0000e-04\n",
      "Epoch 42/50\n",
      "350/350 - 7s - 20ms/step - accuracy: 0.7494 - loss: 0.6622 - val_accuracy: 0.8519 - val_loss: 0.4546 - learning_rate: 2.5000e-04\n",
      "Epoch 43/50\n",
      "350/350 - 7s - 19ms/step - accuracy: 0.7541 - loss: 0.6639 - val_accuracy: 0.8381 - val_loss: 0.4787 - learning_rate: 2.5000e-04\n",
      "Epoch 44/50\n",
      "350/350 - 7s - 19ms/step - accuracy: 0.7488 - loss: 0.6593 - val_accuracy: 0.8425 - val_loss: 0.4610 - learning_rate: 2.5000e-04\n",
      "Epoch 45/50\n",
      "350/350 - 7s - 19ms/step - accuracy: 0.7472 - loss: 0.6570 - val_accuracy: 0.8381 - val_loss: 0.4692 - learning_rate: 2.5000e-04\n",
      "Epoch 46/50\n",
      "350/350 - 7s - 19ms/step - accuracy: 0.7490 - loss: 0.6564 - val_accuracy: 0.8456 - val_loss: 0.4620 - learning_rate: 2.5000e-04\n",
      "Epoch 47/50\n",
      "350/350 - 7s - 21ms/step - accuracy: 0.7535 - loss: 0.6451 - val_accuracy: 0.8462 - val_loss: 0.4597 - learning_rate: 2.5000e-04\n",
      "Epoch 48/50\n",
      "350/350 - 7s - 20ms/step - accuracy: 0.7555 - loss: 0.6325 - val_accuracy: 0.8394 - val_loss: 0.4709 - learning_rate: 1.2500e-04\n",
      "Epoch 49/50\n",
      "350/350 - 7s - 20ms/step - accuracy: 0.7535 - loss: 0.6354 - val_accuracy: 0.8450 - val_loss: 0.4548 - learning_rate: 1.2500e-04\n",
      "Epoch 50/50\n",
      "350/350 - 7s - 21ms/step - accuracy: 0.7579 - loss: 0.6340 - val_accuracy: 0.8494 - val_loss: 0.4451 - learning_rate: 1.2500e-04\n",
      "63/63 - 1s - 8ms/step - accuracy: 0.8465 - loss: 0.4606\n",
      "\n",
      "Test Accuracy: 0.8465\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n"
     ]
    }
   ],
   "source": [
    "## CNN  ##\n",
    "\n",
    "full_df_ANN = pd.read_csv('interp_lc_5sem.csv')\n",
    "X = full_df_ANN.drop(columns=['target']).values\n",
    "y = full_df_ANN['target'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scal = scaler.fit_transform(X)\n",
    "\n",
    "y_enc = pd.get_dummies(y, dtype=int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scal, y_enc, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Conversione da one-hot a etichette numeriche per SMOTE\n",
    "y_train_lab = np.argmax(y_train.values, axis=1)\n",
    "\n",
    "smote = SMOTE(sampling_strategy='not majority', random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train_lab)\n",
    "\n",
    "# Conversione delle etichette riequilibrate in one-hot encoding\n",
    "y_train_res = pd.get_dummies(y_train_res)\n",
    "\n",
    "# Reshape dei dati per Conv1D: (campioni, caratteristiche, 1)\n",
    "X_train_res = X_train_res.reshape((X_train_res.shape[0], X_train_res.shape[1], 1))\n",
    "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Definizione del modello CNN\n",
    "model_cnn1 = Sequential([\n",
    "    Conv1D(filters=32,kernel_size=7, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Conv1D(filters=64, kernel_size=7, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    # Strati densi aggiuntivi\n",
    "    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7)\n",
    "checkpoint = ModelCheckpoint('best_model_cnn_lstm.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "model_cnn1.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_cnn1 = model_cnn1.fit(\n",
    "    x=X_train_res, y=y_train_res,  # Usa X_train_res e y_train_res bilanciati\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    callbacks=[early_stopping, lr_scheduler, checkpoint],\n",
    "    verbose=2)\n",
    "\n",
    "test_loss, test_acc = model_cnn1.evaluate(X_test, y_test, verbose=2)\n",
    "print(f'\\nTest Accuracy: {test_acc:.4f}')\n",
    "\n",
    "y_pred_cnn1 = np.argmax(model_cnn1.predict(X_test), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice di Confusione CNN:\n",
      "[[247   0  22   0   2]\n",
      " [  0 709   0   4   3]\n",
      " [ 42   0 545   2   3]\n",
      " [  1  18  36 128  91]\n",
      " [  3  11   4  54  75]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_performance(model_cnn1, X_test=X_test, y_test=y_test, y_pred=y_pred_cnn1, history=history_cnn1, model_name='CNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rete Neurale Ricorrente (RNN)\n",
    "\n",
    "È una rete neurale profonda addestrata su dati ordinati temporalmente. Questa è la rete per cui si adattano le curve interpolate e non filtrate rispetto ai semestri osservati, perché così si riesce a conservare l'informazione temporale!\n",
    "\n",
    "Tale rete viene suggerita in letteratura per risolvere questo genere di problemi di classificazione multiclasse [[ref](https://academic.oup.com/mnras/article/505/3/4345/6287587?login=false)]\n",
    "\n",
    "<img src=\"progetto/ref1.png\" alt=\"y\" style=\"width: 300px;\"/>\n",
    "\n",
    "Purtroppo, però, questa rete ha richiesto un costo computazionale troppo alto per il mio computer, pertanto non è stato possibile completarne l'esecuzione. Ne viene comunque dato un esempio, che mi riserverò di proporre per completezza di analisi. La necessità di scrivere questa rete trova giustificazione nel fatto che ancora si confondono (sebbene come atteso) le ultime due classi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSIONI\n",
    "\n",
    "I modelli messi in campo per classificare le stelle per mezzo delle loro curve di luce simulate nelle cinque classi in cui si suddividono hanno dato dei buoni risultati, riuscendo a mantenere un'accuratezza superiore all'85%. Anche laddove dalla matrice di confusione si riscontrano predizioni sbagliate, queste appaiono in posizioni che era facile prevedere sarebbero andate peggio, pertanto in linea con quanto atteso. Tuttavia, si osserva che vi sono ancora dei buoni di miglioramento che possono essere sintetizzati in questi passaggi:\n",
    "\n",
    "- compilazione rete RNN e rete CNN+RNN per riuscire meglio a discriminare le ultime due classi;\n",
    "- utilizzo di un dataset da 100,000 curve di luce (recentemente prodotto) più rappresentativo di ciascuna classe;\n",
    "- test con dati reali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_ANN = pd.read_csv('interp_lc_5sem.csv')\n",
    "X = full_df_ANN.drop(columns=['target']).values\n",
    "y = full_df_ANN['target'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scal = scaler.fit_transform(X)\n",
    "\n",
    "y_enc = pd.get_dummies(y, dtype=int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scal, y_enc, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train_lab = np.argmax(y_train.values, axis=1)\n",
    "\n",
    "smote = SMOTE(sampling_strategy='not majority', random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train_lab)\n",
    "\n",
    "y_train_res = pd.get_dummies(y_train_res)\n",
    "\n",
    "X_train_res = X_train_res.reshape((X_train_res.shape[0], X_train_res.shape[1], 1))\n",
    "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    model = Sequential([\n",
    "        GRU(128, return_sequences=True, kernel_regularizer=regularizers.l2(0.01), input_shape=(X_train_res.shape[1], 1)),\n",
    "        Dropout(0.2),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        GRU(128, return_sequences=False, kernel_regularizer=regularizers.l2(0.01)),\n",
    "        Dropout(0.2),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Dense(5, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7)\n",
    "checkpoint = ModelCheckpoint('best_model_gru.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "history = model.fit(\n",
    "    x=X_train_res, y=y_train_res,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=32,\n",
    "    epochs=25,\n",
    "    callbacks=[early_stopping, lr_scheduler, checkpoint],\n",
    "    verbose=2)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f'\\nTest Accuracy: {test_acc:.4f}')\n",
    "y_pred = np.argmax(model.predict(X_test), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogamente alla RNN si può immaginare di scrivere una rete ibrida che faccia uso di entrambe le strategie (CNN+RNN) per un'analisi più precisa, ma anche qua il costo computazionale non è stato gestibile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CNN + RNN ##\n",
    "\n",
    "full_df_ANN = pd.read_csv('interp_lc_5sem.csv')\n",
    "X = full_df_ANN.drop(columns=['target']).values\n",
    "y = full_df_ANN['target'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scal = scaler.fit_transform(X)\n",
    "\n",
    "y_enc = pd.get_dummies(y, dtype=int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scal, y_enc, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train_lab = np.argmax(y_train.values, axis=1)\n",
    "\n",
    "smote = SMOTE(sampling_strategy='not majority', random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train_lab)\n",
    "\n",
    "y_train_res = pd.get_dummies(y_train_res)\n",
    "\n",
    "X_train_res = X_train_res.reshape((X_train_res.shape[0], X_train_res.shape[1], 1))\n",
    "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Definizione del modello CNN+RNN\n",
    "model = Sequential([\n",
    "    Conv1D(filters=32,kernel_size=3, activation='relu', kernel_regularizer=regularizers.l2(0.01), input_shape=(X_train_res.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    GRU(128, return_sequences=True, kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.4),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    GRU(64, return_sequences=False, kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.4),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7)\n",
    "checkpoint = ModelCheckpoint('best_model_cnn_lstm.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    x=X_train_res, y=y_train_res,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    callbacks=[early_stopping, lr_scheduler, checkpoint],\n",
    "    verbose=2)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f'\\nTest Accuracy: {test_acc:.4f}')\n",
    "\n",
    "y_pred = np.argmax(model.predict(X_test), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APPENDICE\n",
    "Qui vengono conservati dei pezzi di codice scritto ma alla fine non utilizzato per idee/strategie poi non utilizzate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 181), (364, 545), (728, 909)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "### QUESTO BLOCCO SERVE A IDENTIFICARE SEPARATAMENTE GLI ARRAY DEI SEMESTRI OSSERVATIVI\n",
    "### utile nel caso di rebinning\n",
    "rest_indices = np.where(seasonal_mask==0)[0]\n",
    "\n",
    "#funzione per trovare i blocchi continui di osservazione attiva\n",
    "def find_active_blocks(seasonal_array):\n",
    "    active_indices = np.where(seasonal_array == 1)[0]\n",
    "    active_blocks = []\n",
    "    start_idx = active_indices[0]\n",
    "    \n",
    "    for i in range(1, len(active_indices)):\n",
    "        if active_indices[i] != active_indices[i - 1] + 1:\n",
    "            active_blocks.append((start_idx, active_indices[i - 1]))\n",
    "            start_idx = active_indices[i]\n",
    "    active_blocks.append((start_idx, active_indices[-1]))\n",
    "    \n",
    "    return active_blocks\n",
    "\n",
    "#trova e stampa i blocchi attivi\n",
    "active_blocks = find_active_blocks(seasonal_mask)\n",
    "print(active_blocks)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "### altro tentativo che avevo fatto per fittare le curve ###\n",
    "#ciò sarebbe stato utile o per trovare grandezze fisiche nel pattern (periodo, fase, ampiezza)\n",
    "#oppure come metodo di interpolazione alternativo ad 'inter1d'\n",
    "\n",
    "y_curve = interpolated_curves[n] # curva di luce n-esima\n",
    "\n",
    "degree = 3 #grado della funzione da fittare\n",
    "coefficients = np.polyfit(x_filt, y_curve[seasonal_mask], degree)\n",
    "poly_func = np.poly1d(coefficients) #funzione polinomiale dai coefficienti\n",
    "\n",
    "# creo un array per tracciare la curva polinomiale\n",
    "x_fit = np.linspace(np.min(x_filt), np.max(x_filt), 100)\n",
    "y_fit = poly_func(x_fit)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_filt, y_curve[seasonal_mask], s=2, color='#ff7f0e', label='Curva di luce originale')\n",
    "plt.plot(x_fit, y_fit, color='red', label=f'Polinomio di grado {degree}')\n",
    "plt.ylim(-1.2, 1.2)\n",
    "plt.xlabel('Tempo')\n",
    "plt.ylabel('Intensità')\n",
    "plt.title(f'Curva di Luce Simulata {n+1} con polyfit di grado {degree}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "### ESPLORAZIONE IPERPARAMETRI PER NN\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from kerastuner import HyperModel, RandomSearch\n",
    "\n",
    "#Keras Tuner\n",
    "class MyHyperModel(HyperModel):\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        model.add(layers.Input(shape=(X_train.shape[1],)))\n",
    "\n",
    "        # Aggiungi i layer densi\n",
    "        for i in range(hp.Int('num_layers', 2, 6)):  # Numero di layer densi\n",
    "            units = hp.Int(f'units_{i}', min_value=64, max_value=512, step=64)\n",
    "            model.add(Dense(units, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "            model.add(Dropout(hp.Float('dropout_' + str(i), 0.2, 0.4, step=0.1)))\n",
    "\n",
    "        model.add(Dense(5, activation='softmax'))  # Output per 5 classi\n",
    "\n",
    "        # Compila il modello\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    MyHyperModel(),\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='my_dir',\n",
    "    project_name='helloworld7'\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "tuner.search(X_train, y_train, \n",
    "             validation_data=(X_val, y_val), \n",
    "             epochs=10, \n",
    "             callbacks=[early_stopping, lr_scheduler, checkpoint], \n",
    "             verbose=2)\n",
    "\n",
    "#ritorna il miglior modello\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "#stampa la migliore combinazione di parametri\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Migliore combinazione di parametri:\")\n",
    "print(f\"Numero di layer: {best_hyperparameters.get('num_layers')}\")\n",
    "for i in range(best_hyperparameters.get('num_layers')):\n",
    "    print(f\"Layer {i + 1}: Units = {best_hyperparameters.get(f'units_{i}')}, Dropout = {best_hyperparameters.get(f'dropout_{i}')}\")\n",
    "\n",
    "test_loss, test_acc = best_model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f'\\nTest Accuracy: {test_acc:.4f}')\n",
    "\n",
    "y_pred = np.argmax(best_model.predict(X_test), axis=-1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#numero di alberi ottimale per random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#array per i numeri di alberi da testare\n",
    "n_estimators = np.arange(1, 201, 10)  # Testiamo da 1 a 200 alberi con incrementi di 10\n",
    "scores = []  # Per memorizzare le performance\n",
    "\n",
    "#ciclo per calcolare le performance\n",
    "for n in n_estimators:\n",
    "    clf = RandomForestClassifier(n_estimators=n, random_state=42, n_jobs=-1)\n",
    "    # Eseguiamo la validazione incrociata e calcoliamo la media dei punteggi\n",
    "    score = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')  # Usando accuracy come metrica\n",
    "    scores.append(score.mean())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_estimators, scores, marker='o')\n",
    "plt.title('Performance del Random Forest in funzione del numero di alberi')\n",
    "plt.xlabel('Numero di Alberi')\n",
    "plt.ylabel('Accuratezza Media (CV)')\n",
    "plt.grid(True)\n",
    "plt.xticks(n_estimators)\n",
    "plt.show()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
